return(results)
}
stump <- function(x,y,w,xpred=NULL){
# A stump model for classification with just one split
# for one dimensional problem.
# @parameters:
# x: feature vector; n*1
# y: label vector; n*1
# w: weights vector; n*1
# xpred: points to be predicted
# @values:
# cut_point: cutting poings; 1*1
# fval_left: left node predictions; character
# fval_right: right node predictions; character
Gini <- function(y0,w0){
# calculate weighted Gini index for curret node
# @values:
# return the Gini Index for current node
p <- (t(w0) %*% (y0==1)) / (sum(w0))
return(p * (1 - p))
}
y <- y[order(x)]
w <- w[order(x)]
x <- x[order(x)]
xrange <-unique(x); xrange <- xrange[-length(xrange)]
left <- rep(NA,length(xrange)); right <- left
score_mat <- matrix(NA,nrow=length(xrange),ncol=2)
count <- 0
for (c in xrange){
split_idx <- max(which(x==c))
y_left <- y[1:split_idx]; y_right <- y[-c(1:split_idx)]
w_left <- w[1:split_idx]; w_right <- w[-c(1:split_idx)]
Gini_left <- Gini(y_left, w_left); Gini_right <- Gini(y_right, w_right)
count <- count + 1
score_mat[count,1] <- sum(w_left) / sum(w) * Gini_left +
sum(w_right) / sum(w) * Gini_right
score_mat[count,2] <- c # cut point
p.left <- sum(w_left[y_left==1])/sum(w_left)
p.right <- sum(w_right[y_right==1])/sum(w_right)
left[count] <- ifelse(p.left >= 0.5, 1, -1)
right[count] <- ifelse(p.right >= 0.5, 1, -1)
}
idx <- which.min(score_mat[,1])
c <- xrange[idx]
fval_left <- left[idx]
fval_right <- right[idx]
if (is.null(xpred) == TRUE){
results <- list(cut_point=c,fval_left=fval_left,
fval_right=fval_right)
}else{
ypred <- (xpred <= c) * fval_left + (xpred > c) * fval_right
results <- list(cut_point=c,fval_left=fval_left,
fval_right=fval_right,
xpred=xpred,ypred=ypred)
}
return(results)
}
r <- boosting.stump(x,y,xpred=x,ytest=y,iterations=300,shrinkage = 1)
orderx <- order(x)
x <- x[order(x)]
py <-  (sin (4*pi*x)+1)/2
pr <- r$ypred.prob
par(mfrow=c(2,3))
size <- c(1,10,50,100,150,300)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
py <-  (sin (4*pi*x)+1)/2
pr <- r$ypred.prob
par(mfrow=c(2,3))
size <- c(1,10,50,100,150,300)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
save(r,file="prob_Q3.Rdata")
rm(r)
load("prob_Q3.Rdata")
orderx <- order(x)
x <- x[order(x)]
py <-  (sin (4*pi*x)+1)/2
pr <- r$ypred.prob
par(mfrow=c(2,3))
size <- c(1,10,50,100,150,300)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
n = 300
set.seed(1234)
x = runif(n)
y = (rbinom(n , 1 , (sin (4*pi*x)+1)/2)-0.5)*2
set.seed(5678)
ntest = 100
xpred = runif(ntest)
ytest = (rbinom(ntest , 1 , (sin (4*pi*xpred)+1)/2)-0.5)*2
orderx <- order(x)
x <- x[order(x)]
py <-  (sin (4*pi*x)+1)/2
pr <- r$ypred.prob
par(mfrow=c(2,3))
size <- c(1,10,50,100,150,300)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
x1 <- c(1,2,3,4,5,6)
y1 <- c(-1,-1,-1,1,1,-1)
w1 <- rep(1/length(x1),length(x1))
stump(x1,y1,w1,x1)
x2 <- c(1,2,3,4,5,6)
y2 <- c(1,-1,-1,1,1,-1)
w2 <- c(10,1,1,1,1,10)
stump(x2,y2,w2,x2)
fit2 <- stump(x1,y1,w1,x1)
plot(x2,x2,pch=w2)
plot(x2,y2,pch=w2)
?pch
plot(x2,y2,cex=w2)
plot(x2,y2,cex=w2/sum(w2))
plot(x2,y2,cex=w2/sum(w2)*2)
plot(x2,y2,cex=w2/sum(w2)*2,col=y2)
plot(x2,y2,cex=w2/sum(w2)*2,col=y2+3)
x2 <- c(1,2,3,4,5,6)
y2 <- c(1,-1,-1,1,1,-1)
w2 <- c(3,1,1,1,1,3)
fit2 <- stump(x2,y2,w2,x2)
plot(x2,y2,cex=w2/sum(w2)*2,col=y2+3)
abline(v=fit2$cut_point)
?jitter
plot(x2,y2,cex=w2/sum(w2)*2,col=y2+3)
abline(v=fit2$cut_point+0.2)
x2 <- c(1,2,3,4,5,6)
y2 <- c(1,-1,-1,1,1,-1)
w2 <- c(3,1,1,1,1,3)
fit2 <- stump(x2,y2,w2,x2)
plot(x2,y2,cex=w2/sum(w2)*2,col=y2+3)
abline(v=fit2$cut_point*1.1)
x1 <- c(1,2,3,4,5,6)
y1 <- c(-1,-1,-1,1,1,-1)
w1 <- rep(1/length(x1),length(x1))
fit1 <- stump(x1,y1,w1,x1)
# find how weights influence the decision
w2 <- c(3,1,1,1,1,3)
fit2 <- stump(x1,y1,w2,x1)
par(mfrow=c(2,3))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3)
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3)
abline(v=fit2$cut_point*1.1)
par(mfrow=c(1,2))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3)
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3)
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3)
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3)
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(0, 4, 4, 2))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(0, 2, 2, 2))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mar=c(0, 2, 2, 1))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*2,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*5,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*5,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
title("An Toy Example")
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
title("An Toy Example")
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
x1 <- c(1,2,3,4,5,6)
y1 <- c(-1,-1,-1,1,1,-1)
w1 <- rep(1/length(x1),length(x1))
fit1 <- stump(x1,y1,w1,x1)
# find how weights influence the decision
w2 <- c(3,1,1,1,1,3)
fit2 <- stump(x1,y1,w2,x1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
x1 <- c(1,2,3,4,5,6)
y1 <- c(-1,-1,-1,1,1,-1)
w1 <- rep(1/length(x1),length(x1))
fit1 <- stump(x1,y1,w1,x1)
# find how weights influence the decision
w2 <- c(5,1,1,1,1,5)
fit2 <- stump(x1,y1,w2,x1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
x1 <- c(1,2,3,4,5,6)
y1 <- c(1,-1,-1,1,1,-1)
w1 <- rep(1/length(x1),length(x1))
fit1 <- stump(x1,y1,w1,x1)
# find how weights influence the decision
w2 <- c(3,1,1,1,1,3)
fit2 <- stump(x1,y1,w2,x1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
x1 <- c(1,2,3,4,5,6,7)
y1 <- c(1,-1,-1,1,1,-1,1)
w1 <- rep(1/length(x1),length(x1))
fit1 <- stump(x1,y1,w1,x1)
# find how weights influence the decision
w2 <- c(3,1,1,1,1,3)
fit2 <- stump(x1,y1,w2,x1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
x1 <- c(1,2,3,4,5,6,7)
y1 <- c(1,-1,-1,1,1,-1,1)
w1 <- rep(1/length(x1),length(x1))
fit1 <- stump(x1,y1,w1,x1)
# find how weights influence the decision
w2 <- c(3,1,1,1,1,3,1)
fit2 <- stump(x1,y1,w2,x1)
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
fit1$fval_left
fit2$fval_left
text(x= 2, y=0, as.character(fit2$fval_right))
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
text(x= 2, y=0, as.character(fit1$fval_left))
text(x= 5.5, y=0, as.character(fit1$fval_right))
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.1)
text(x= 0.5, y=0, as.character(fit2$fval_left))
text(x= 4, y=0, as.character(fit2$fval_right))
text(x= 0.1, y=0, as.character(fit2$fval_left))
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
text(x= 2, y=0, as.character(fit1$fval_left))
text(x= 5.5, y=0, as.character(fit1$fval_right))
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.5)
text(x= 0.1, y=0, as.character(fit2$fval_left))
text(x= 4, y=0, as.character(fit2$fval_right))
fit2$fval_left
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.1)
text(x= 2, y=0, as.character(fit1$fval_left))
text(x= 5.5, y=0, as.character(fit1$fval_right))
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.5)
text(x= 1, y=0, as.character(fit2$fval_left))
text(x= 4, y=0, as.character(fit2$fval_right))
par(mfrow=c(2,1))
par(mar=c(2,2,3,1))
plot(x1,y1,cex=w1/sum(w1)*3,col=y1+3,xlab = "")
abline(v=fit1$cut_point*1.5)
text(x= 2, y=0, as.character(fit1$fval_left))
text(x= 5.5, y=0, as.character(fit1$fval_right))
title("An Toy Example")
plot(x1,y1,cex=w2/sum(w2)*3,col=y1+3,xlab = "")
abline(v=fit2$cut_point*1.5)
text(x= 1, y=0, as.character(fit2$fval_left))
text(x= 4, y=0, as.character(fit2$fval_right))
r <- boosting.stump(x,y,xpred=x,ytest=y,iterations=1500,shrinkage = 1)
stump <- function(x,y,w,xpred=NULL){
# A stump model for classification with just one split
# for one dimensional problem.
# @parameters:
# x: feature vector; n*1
# y: label vector; n*1
# w: weights vector; n*1
# xpred: points to be predicted
# @values:
# cut_point: cutting poings; 1*1
# fval_left: left node predictions; character
# fval_right: right node predictions; character
Gini <- function(y0,w0){
# calculate weighted Gini index for curret node
# @values:
# return the Gini Index for current node
p <- (t(w0) %*% (y0==1)) / (sum(w0))
return(p * (1 - p))
}
y <- y[order(x)]
w <- w[order(x)]
x <- x[order(x)]
xrange <-unique(x); xrange <- xrange[-length(xrange)]
left <- rep(NA,length(xrange)); right <- left
score_mat <- matrix(NA,nrow=length(xrange),ncol=2)
count <- 0
for (c in xrange){
split_idx <- max(which(x==c))
y_left <- y[1:split_idx]; y_right <- y[-c(1:split_idx)]
w_left <- w[1:split_idx]; w_right <- w[-c(1:split_idx)]
Gini_left <- Gini(y_left, w_left); Gini_right <- Gini(y_right, w_right)
count <- count + 1
score_mat[count,1] <- sum(w_left) / sum(w) * Gini_left +
sum(w_right) / sum(w) * Gini_right
score_mat[count,2] <- c # cut point
p.left <- sum(w_left[y_left==1])/sum(w_left)
p.right <- sum(w_right[y_right==1])/sum(w_right)
left[count] <- ifelse(p.left >= 0.5, 1, -1)
right[count] <- ifelse(p.right >= 0.5, 1, -1)
}
idx <- which.min(score_mat[,1])
c <- xrange[idx]
fval_left <- left[idx]
fval_right <- right[idx]
if (is.null(xpred) == TRUE){
results <- list(cut_point=c,fval_left=fval_left,
fval_right=fval_right)
}else{
ypred <- (xpred <= c) * fval_left + (xpred > c) * fval_right
results <- list(cut_point=c,fval_left=fval_left,
fval_right=fval_right,
xpred=xpred,ypred=ypred)
}
return(results)
}
boosting.stump <- function(x,y,xpred,ytest=NULL,iterations=100,shrinkage=1){
# Boosting tree using the stump classifier
# for one dimensional problem.
# @parameters:
# x: feature vector; n*1; training
# y: label vector; n*1; training
# xpred: points to be predicted
# ytest: prediction error is given, if provided
# iterations: total number of base learner
# shrinkage: further decrease the weight for each learner
# @values:
# ypred.mat: prediction matrix; n*iternation; cummulative predicition
#            e.g, the column i represents the predicted value base on
#            first i base learners's prediction results
# pred.error: prediciton error v.s iteration (availabe if ytest provided.)
w <- rep(1 / length(x), length(x))
error.mat <- rep(NA,iterations)
boosting.pred <- matrix(NA,ncol=iterations,nrow=length(xpred))
for (t in seq_len(iterations)){
learner.t <- stump(x,y,w,x)
stump.fitted.t <- learner.t$ypred
epsilon.t <- t(w) %*% (stump.fitted.t != y)
alpha.t <- 0.5 * log( (1 - epsilon.t) / epsilon.t) * shrinkage
w <- w * exp( c(-alpha.t) * y * stump.fitted.t) # update weight
w <- w / sum(w) # normalize weight
left <- learner.t$fval_left
right <- learner.t$fval_right
cut_point <- learner.t$cut_point
stump.fitted.t <- (x<=cut_point) * left + (x>cut_point) * right
boosting.pred[,t] <- c(alpha.t) * ((xpred<=cut_point) * left +
(xpred>cut_point) * right)
}
ypred.mat <- t(apply(boosting.pred,1,cumsum))
ypred.prob <- apply(ypred.mat,2,function(Fx) 1/(1+exp(-2*Fx)))
ypred.mat <- sign(ypred.mat)
if (is.null(ytest) == TRUE){
results <- list(ypred.prob,
ypred.mat=ypred.mat)
}else{
pred.error <- rep(NA,length(xpred))
for (i in seq_len(ncol(ypred.mat))){
pred.error[i] <- sum(ytest!=ypred.mat[,i]) / length(ytest)
}
results <- list(ypred.prob=ypred.prob,
ypred.mat=ypred.mat,
pred.error=pred.error)
}
return(results)
}
n = 300
set.seed(1234)
x = runif(n)
y = (rbinom(n , 1 , (sin (4*pi*x)+1)/2)-0.5)*2
r <- boosting.stump(x,y,xpred=x,ytest=y,iterations=1500,shrinkage = 1)
orderx <- order(x)
x <- x[order(x)]
py <-  (sin (4*pi*x)+1)/2
pr <- r$ypred.prob
par(mfrow=c(2,3))
size <- c(1,10,50,500,1000,1500)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
par(mfrow=c(2,3))
size <- c(1,10,50,500,800,1500)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
pr <- r$ypred.prob
par(mfrow=c(2,3))
size <- c(1,10,50,300,800,1000)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
size <- c(1,10,50,300,800,1200)
for(i in 1:6)
{
par(mar=c(2,2,3,1))
plot(x, py, type = "l", lwd = 3, ylab = "P(Y=1 | X=x)", col = "gray")
lines(x, (pr[,size[i]])[orderx], lwd = 2,col= "deepskyblue")
abline(h=0.5,lty=2,col="darkorange",lwd = 2)
title(paste("# of Iterations = ", size[i]))
}
save(r,file="prob_Q3.Rdata")
