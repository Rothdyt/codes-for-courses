{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File: RNN_language_model.py\n",
    "Author: Yutong Dai (rothdyt@gmail.com)\n",
    "File Created: Saturday, 2018-11-08 20:01\n",
    "Last Modified: Thursday, 2018-11-08 20:59\n",
    "--------------------------------------------\n",
    "Desscription:\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed as dist\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "\n",
    "class StatefulLSTM(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(StatefulLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTMCell(in_size, out_size)\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.data.size()[0]\n",
    "        if self.h is None:\n",
    "            state_size = [batch_size, self.out_size]\n",
    "            self.c = Variable(torch.zeros(state_size))\n",
    "            self.h = Variable(torch.zeros(state_size))\n",
    "        self.h, self.c = self.lstm(x, (self.h, self.c))\n",
    "\n",
    "        return self.h\n",
    "\n",
    "\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LockedDropout, self).__init__()\n",
    "        self.m = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.m = None\n",
    "\n",
    "    def forward(self, x, dropout=0.5, train=True):\n",
    "        if train == False:\n",
    "            return x\n",
    "        if(self.m is None):\n",
    "            self.m = x.data.new(x.size()).bernoulli_(1 - dropout)\n",
    "        mask = Variable(self.m, requires_grad=False) / (1 - dropout)\n",
    "\n",
    "        return mask * x\n",
    "\n",
    "\n",
    "class RNN_language_model(nn.Module):\n",
    "    def __init__(self, vocab_size, no_of_hidden_units):\n",
    "        super(RNN_language_model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, no_of_hidden_units)\n",
    "\n",
    "        self.lstm1 = StatefulLSTM(no_of_hidden_units, no_of_hidden_units)\n",
    "        self.bn_lstm1 = nn.BatchNorm1d(no_of_hidden_units)\n",
    "        self.dropout1 = LockedDropout()\n",
    "\n",
    "\n",
    "        # distribution on the whole dictionary\n",
    "        self.decoder = nn.Linear(no_of_hidden_units, vocab_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm1.reset_state()\n",
    "        self.dropout1.reset_state()\n",
    "\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "\n",
    "        embed = self.embedding(x)  # batch_size, time_steps, features\n",
    "\n",
    "        no_of_timesteps = embed.shape[1]-1\n",
    "\n",
    "        self.reset_state()\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(no_of_timesteps):\n",
    "\n",
    "            h = self.lstm1(embed[:, i, :])\n",
    "            h = self.bn_lstm1(h)\n",
    "            h = self.dropout1(h, dropout=0.3, train=train)\n",
    "\n",
    "\n",
    "            h = self.decoder(h)\n",
    "\n",
    "            outputs.append(h)\n",
    "\n",
    "        outputs = torch.stack(outputs)  # (time_steps,batch_size,vocab_size)\n",
    "        target_prediction = outputs.permute(1, 0, 2)  # batch, time, vocab\n",
    "        outputs = outputs.permute(1, 2, 0)  # (batch_size,vocab_size,time_steps)\n",
    "\n",
    "        if(train == True):\n",
    "\n",
    "            target_prediction = target_prediction.contiguous().view(-1, self.vocab_size)\n",
    "            target = x[:, 1:].contiguous().view(-1)\n",
    "            loss = self.loss(target_prediction, target)\n",
    "\n",
    "            return loss, outputs\n",
    "        else:\n",
    "            return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "x_train = []\n",
    "with io.open('../../data/nlp/imdb_train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = line.split(' ')\n",
    "    line = np.asarray(line, dtype=np.int)\n",
    "    # convert any token id greater than the dictionary size to the unknown token ID 0\n",
    "    line[line > vocab_size] = 0\n",
    "    x_train.append(line)\n",
    "\n",
    "x_train = x_train[0:25000]\n",
    "y_train = np.zeros((25000,))\n",
    "# positive label\n",
    "y_train[0:12500] = 1\n",
    "\n",
    "# for unknown token\n",
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x_input2 = [x_train[j] for j in [0,1,2,4]]\n",
    "sequence_length = 50\n",
    "x_input = np.zeros((batch_size,sequence_length),dtype=np.int)\n",
    "for j in range(batch_size):\n",
    "    x = np.asarray(x_input2[j])\n",
    "    sl = x.shape[0]\n",
    "    if(sl<sequence_length):\n",
    "        x_input[j,0:sl] = x\n",
    "    else:\n",
    "        start_index = np.random.randint(sl-sequence_length+1)\n",
    "        x_input[j,:] = x[start_index:(start_index+sequence_length)]\n",
    "x_input = Variable(torch.LongTensor(x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_language_model(vocab_size, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 200])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_input\n",
    "embed = model.embedding(x)  # batch_size, time_steps, features\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_timesteps = embed.shape[1]-1\n",
    "\n",
    "model.reset_state()\n",
    "\n",
    "outputs = []\n",
    "for i in range(no_of_timesteps):\n",
    "\n",
    "    h = model.lstm1(embed[:, i, :])\n",
    "    h = model.bn_lstm1(h)\n",
    "    h = model.dropout1(h, dropout=0.3, train=True)\n",
    "\n",
    "\n",
    "    h = model.decoder(h)\n",
    "\n",
    "    outputs.append(h)\n",
    "\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8001])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8001, 49])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = torch.stack(outputs)  # (time_steps,batch_size,vocab_size)\n",
    "target_prediction = outputs.permute(1, 0, 2)  # batch, time, vocab\n",
    "outputs = outputs.permute(1, 2, 0)  # (batch_size,vocab_size,time_steps)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49, 8001])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_prediction = target_prediction.contiguous()#.view(-1, model.vocab_size)\n",
    "target_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = x[:, 1:].contiguous()#.view(-1)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dictionary = np.load('../../data/nlp/imdb_dictionary.npy')\n",
    "vocab_size = 8000 + 1\n",
    "\n",
    "word_to_id = {token: idx for idx, token in enumerate(imdb_dictionary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_language_model(vocab_size, 500)\n",
    "tokens = [['yes'], ['i']]\n",
    "token_ids = np.asarray([[word_to_id.get(token,-1)+1 for token in x] for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# create partial sentences to \"prime\" the model\n",
    "# this implementation requires the partial sentences\n",
    "# to be the same length if doing more than one\n",
    "# tokens = [['i','love','this','movie','.'],['i','hate','this','movie','.']]\n",
    "\n",
    "\n",
    "\n",
    "def genearate_review(tokens, temperature, model, word_to_id):\n",
    "    token_ids = np.asarray([[word_to_id.get(token, -1)+1 for token in x] for x in tokens])\n",
    "    # preload phrase\n",
    "    x = Variable(torch.LongTensor(token_ids))\n",
    "\n",
    "    embed = model.embedding(x)  # batch_size, time_steps, features; 2, 1, 500\n",
    "\n",
    "    no_of_timesteps = embed.shape[1]\n",
    "\n",
    "    model.reset_state()\n",
    "    token_ids = np.asarray([[word_to_id.get(token, -1)+1 for token in x] for x in tokens])\n",
    "    # preload phrase\n",
    "    x = Variable(torch.LongTensor(token_ids))\n",
    "    embed = model.embedding(x)  # batch_size, time_steps, features; 2, 1, 500\n",
    "    outputs = []\n",
    "    for i in range(no_of_timesteps):\n",
    "        h = model.lstm1(embed[:, i, :])  # input, batch_size*features, 2*500, output, 2*500\n",
    "        h = model.bn_lstm1(h)\n",
    "        h = model.dropout1(h, dropout=0.3, train=False)\n",
    "        h = model.decoder(h)\n",
    "        outputs.append(h)\n",
    "\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = outputs.permute(1, 2, 0)\n",
    "    output = outputs[:, :, -1]  # batch_size, vocab_size\n",
    "\n",
    "    length_of_review = 10\n",
    "\n",
    "    review = []\n",
    "    ####\n",
    "    for _ in range(length_of_review):\n",
    "\n",
    "        # sample a word from the previous output\n",
    "        output = output/temperature\n",
    "        probs = torch.exp(output)\n",
    "        probs[:, 0] = 0.0  # discard unknown token\n",
    "        probs = probs/(torch.sum(probs, dim=1).unsqueeze(1))\n",
    "        x = torch.multinomial(probs, 1)\n",
    "        review.append(x.cpu().data.numpy()[:, 0])\n",
    "\n",
    "        # predict the very next word\n",
    "        embed = model.embedding(x)\n",
    "\n",
    "        h = model.lstm1(embed[:, 0, :])\n",
    "        h = model.bn_lstm1(h)\n",
    "        h = model.dropout1(h, dropout=0.3, train=False)\n",
    "\n",
    "        output = model.decoder(h)\n",
    "\n",
    "    review = np.asarray(review)\n",
    "    review = review.T\n",
    "    review = np.concatenate((token_ids, review), axis=1)\n",
    "    review = review - 1\n",
    "    review[review < 0] = vocab_size - 1\n",
    "    review_words = imdb_dictionary[review]\n",
    "    for review in review_words:\n",
    "        prnt_str = ''\n",
    "        for word in review:\n",
    "            prnt_str += word\n",
    "            prnt_str += ' '\n",
    "        print(prnt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes misguided deliver tears sensational solid hopeful offered concerned daily example \n",
      "i armed amazingly mason high iv discovery well-done milk melancholy invited \n"
     ]
    }
   ],
   "source": [
    "genearate_review(tokens, temperature=1, model=model, word_to_id=word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in review_words:\n",
    "        prnt_str = ''\n",
    "for word in review:\n",
    "        prnt_str += word\n",
    "        prnt_str += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i shorts gigantic deliciously segment ugh ladd nifty creator careful pitt '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prnt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['yes'], ['i'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0], tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, model,  sequence_length=100, batch_size=200, no_of_epochs=20, train_layer=\"last\", LR=0.001):\n",
    "    #logger.info(\"[Train] | train seq_length:{} | train_layer:{} | Epochs:{} | Batch Size:{}\".format(sequence_length, train_layer, no_of_epochs, batch_size))\n",
    "    #model.cuda()\n",
    "    params = []\n",
    "    if (train_layer == \"last\"):\n",
    "        print(\"here\")\n",
    "    else:\n",
    "        print(there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "train(x_train=1, y_train=2, model=3,  sequence_length=100, batch_size=200, no_of_epochs=20, train_layer=\"last\", LR=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
