{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## Forward-Propagation\n",
    "\n",
    "$$\n",
    "X \\rightarrow Z=WX+b_1 \\rightarrow H=\\sigma(Z) \\rightarrow U = CH + b_2\\rightarrow S=F_{softmax}(U) \\rightarrow \\rho(S, y) = \\log S_y,\n",
    "$$\n",
    "where $S_y=\\frac{\\exp(U_y)}{\\sum_{j=0}^{K-1}\\exp(U_j)}$ is the y-th element of the $S$ and $U_y$ is the y-th element of the $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-Propagation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial U_t} =  \n",
    "\\begin{cases}\n",
    "S_t(U), & t\\neq y \\\\\n",
    "1- S_t(U). & t = y\n",
    "\\end{cases}\n",
    "\\Longrightarrow\n",
    "\\frac{\\partial \\rho}{\\partial U} = e_y - S(U), \n",
    "$$\n",
    "\n",
    "where $e_y$ is the unit vector, which y-th coordinate equals to 1 and 0 elsewhere. \n",
    "\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial \\rho}{\\partial b_2} = \\frac{\\partial \\rho}{\\partial U}\\frac{\\partial U}{\\partial b_2} = e_y - S(U) \\\\\n",
    "& \\frac{\\partial \\rho}{\\partial C} = \\frac{\\partial \\rho}{\\partial U}\\frac{\\partial U}{\\partial C} = (e_y - S(U))H^T \\\\\n",
    "& \\frac{\\partial \\rho}{\\partial H} = \\frac{\\partial \\rho}{\\partial U}\\frac{\\partial U}{\\partial H} = C^T\\frac{\\partial \\rho}{\\partial U} =C^T(e_y - S(U)) \\\\\n",
    "& \\frac{\\partial \\rho}{\\partial b_1} = \\frac{\\partial \\rho}{\\partial H}\\frac{\\partial H}{\\partial Z}\\frac{\\partial Z}{\\partial b_1} = \\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z)\\\\\n",
    "& \\frac{\\partial \\rho}{\\partial W} =  \\big(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z)\\big)X^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Mini-Batch Stochastic gradient algorithm for updating $\\theta = \\{W, b_1, C, b_2\\}$:\n",
    "* Step1: Specify batch_size $M$, activation function $\\sigma(z)$, and initialize $W^{(0)}, b_1^{(0)}, C^{(0)}, b_2^{(0)}$;\n",
    "* Step2: At iteration $t$:\n",
    "    * a. Select $M$ data samples $\\{X^{(t,m)},y^{(t,m)}\\}_{m=1}^M$ uniform at random from the full dataset $\\{X^{(n)},y^{(n)}\\}_{n=1}^N$ \n",
    "    * b. Compute forward-propagation:\n",
    "        * $Z^{(t,m)}=W^{(t)}X^{(t,m)}+b_1^{(t)}$\n",
    "        * $H^{(t,m)}=\\sigma(Z^{(t,m)})$\n",
    "        * $U^{(t,m)} = C^{(t)}H^{(t,m)} + b_2^{(t)}$\n",
    "        * $S^{(t,m)}=F_{softmax}(U^{(t,m)})$\n",
    "    * c. Compute backward-propagation:\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_2} = \\frac{1}{M}\\sum_{m=1}^M e_{y^{(t,m)}} - S^{(t,m)}$ \n",
    "        * $\\frac{\\partial \\rho}{\\partial C} =   \\frac{1}{M}\\sum_{m=1}^M  (e_{y^{(t,m)}}  - S^{(t,m)}){H^{(t,m)}}^T$\n",
    "        * $\\frac{\\partial \\rho}{\\partial H} = \\frac{1}{M}\\sum_{m=1}^M C^T(e_{y^{(t,m)}} - S^{(t,m)})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_1} = \\frac{1}{M}\\sum_{m=1}^M \\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t,m)})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial W} =  \\frac{1}{M}\\sum_{m=1}^M \\big(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t,m)})\\big){X^{(t,m)}}^T$\n",
    "    * Given learning rate $\\eta_t$, update parameters as follows:\n",
    "        * $b_2^{(t+1)}) \\leftarrow b_2^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_2}$\n",
    "        * $C^{(t+1)}) \\leftarrow C^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial C}$\n",
    "        * $b_1^{(t+1)}) \\leftarrow b_1^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_1}$\n",
    "        * $W^{(t+1)}) \\leftarrow W^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial W}$\n",
    "* Step3: Repeat Step2 until some convergence criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid unnecessary `for-loop` we can vectoruize the above algorithm. \n",
    "\n",
    "* Step1: Specify batch_size $M$, activation function $\\sigma(z)$, and initialize $W^{(0)}, b_1^{(0)}, C^{(0)}, b_2^{(0)}$;\n",
    "* Step2: At iteration $t$:\n",
    "    * a. Select $M$ data samples $\\{X^{(t,m)},y^{(t,m)}\\}_{m=1}^M$ uniform at random from the full dataset $\\{X^{(n)},y^{(n)}\\}_{n=1}^N$ \n",
    "    * b. Compute forward-propagation:\n",
    "        * $Z^{(t)}=W^{(t)}X^{(t)}+b_1^{(t)}$, where $X^{(t)} = (X^{(t,1)},...,X^{(t,M)})$ and the summation on $b_1$ will be column-wise.\n",
    "        * $H^{(t)}=\\sigma(Z^{(t)})$, where $H^{(t)} = (H^{(t,1)},...,H^{(t,M)})$ and $\\sigma(.)$ is element wise operation.\n",
    "        * $U^{(t)} = C^{(t)}H^{(t)} + b_2^{(t)}$\n",
    "        * $S^{(t)}=F_{softmax}(U^{(t)})$, where the $F_{softmax}$ is column-wise operation.\n",
    "    * c. Compute backward-propagation:\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_2} = \\text{np.mean}(e_{y^{(t)}} - S^{(t)}, \\text{axis=1})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial C} =  \\frac{1}{M} (e_{y^{(t)}}  - S^{(t)}){H^{(t)}}^T$\n",
    "        * $\\frac{\\partial \\rho}{\\partial H} = \\text{np.mean}(C^T(e_{y^{(t)}} - S^{(t)}), \\text{axis=1})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_1} = \\text{np.mean}(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t)}), \\text{axis=1})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial W} =  \\frac{1}{M}(\\big(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t)})\\big){X^{(t)}}^T)$\n",
    "    * Given learning rate $\\eta_t$, update parameters as follows:\n",
    "        * $b_2^{(t+1)}) \\leftarrow b_2^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_2}$\n",
    "        * $C^{(t+1)}) \\leftarrow C^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial C}$\n",
    "        * $b_1^{(t+1)}) \\leftarrow b_1^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_1}$\n",
    "        * $W^{(t+1)}) \\leftarrow W^{(t)} + \\eta_t \\frac{\\partial \\rho}{\\partial W}$\n",
    "* Step3: Repeat Step2 until some convergence criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time \n",
    "import copy\n",
    "#import logging\n",
    "#from helperfunctions import create_log\n",
    "#logger = create_log(file_name=\"task.log\", log_level=logging.DEBUG)\n",
    "file_name = \"../data/MNISTdata.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.info(\"Load the MNIST dataset...\")\n",
    "data = h5py.File(file_name, \"r\")\n",
    "x_train = np.float32(data[\"x_train\"][:])\n",
    "y_train = np.int32(np.hstack(np.array(data[\"y_train\"])))\n",
    "x_test = np.float32(data[\"x_test\"][:])\n",
    "y_test = np.int32(np.hstack(np.array(data[\"y_test\"])))\n",
    "data.close()\n",
    "#logger.info(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel():\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, hidden_units=100, learning_rate=0.01, batch_size=20, num_epochs=5, seed=None):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.num_inputs = self.x_train.shape[1]\n",
    "        self.num_outputs = 10\n",
    "        self.hidden_units = hidden_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.params = {}\n",
    "        self.gradients = {}\n",
    "        if seed is not None:\n",
    "            r = np.random.RandomState(seed)\n",
    "            self.params[\"W\"] = r.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))  \n",
    "            self.params[\"C\"] = r.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1)) \n",
    "        else:\n",
    "            self.params[\"W\"] = np.random.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))\n",
    "            self.params[\"C\"] = np.random.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1))\n",
    "        print(\"training sample size: [{}]\\ntest sample size:[{}]\\nhidden units number: [{}]\\nbatch_size:[{}]\".format(self.x_train.shape, self.x_test.shape, self.hidden_units, self.batch_size))\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: x if x > 0 else 0, z)]\n",
    "\n",
    "    def activation_gradient(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: 1 if x > 0 else 0, z)]\n",
    "\n",
    "    def softmax(self, U):\n",
    "        temp = np.exp(U)\n",
    "        return temp / np.sum(temp)\n",
    "\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        random_index = np.random.choice(self.x_train.shape[0], replace=False, size=self.batch_size)\n",
    "        self.x_train_sub_samples = self.x_train[random_index].reshape((-1, self.batch_size))\n",
    "        self.y_train_sub_samples = self.y_train[random_index]\n",
    "        self.forward_results = {}\n",
    "        self.forward_results[\"Z\"] = np.dot(self.params[\"W\"], self.x_train_sub_samples) + self.params[\"b1\"]\n",
    "        self.forward_results[\"H\"] = np.apply_along_axis(self.activation, 0, self.forward_results[\"Z\"])\n",
    "        self.forward_results[\"U\"] = np.dot(self.params[\"C\"], self.forward_results[\"H\"]) + self.params[\"b2\"]\n",
    "        self.forward_results[\"S\"] = np.apply_along_axis(self.softmax, 0, self.forward_results[\"U\"])\n",
    "\n",
    "    def create_unit_matrix(self):\n",
    "        ey = np.zeros((self.num_outputs, self.batch_size))\n",
    "        for col_index, row_index in enumerate(self.y_train_sub_samples):\n",
    "            ey[row_index, col_index] = 1\n",
    "        return(ey)\n",
    "\n",
    "    def back_propagation(self):\n",
    "        ey = self.create_unit_matrix()\n",
    "        temp = - (ey - self.forward_results[\"S\"])\n",
    "        self.gradients[\"db2\"] = np.mean(temp, axis=1, keepdims=True)\n",
    "        self.gradients[\"dC\"] = np.dot(temp, self.forward_results[\"H\"].T) / self.batch_size\n",
    "        self.gradients[\"dH\"] = np.mean(np.dot(self.params[\"C\"].T, temp), axis=1, keepdims=True)\n",
    "        H_gradient = np.apply_along_axis(self.activation_gradient, 0, self.forward_results[\"Z\"])\n",
    "        temp2 = np.multiply(self.gradients[\"dH\"], H_gradient)\n",
    "        self.gradients[\"db1\"] = np.mean(temp2, axis=1, keepdims=True)\n",
    "        self.gradients[\"dW\"] = np.dot(temp2, self.x_train_sub_samples.T) / self.batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if (epoch > 5):\n",
    "                self.learning_rate = 0.001\n",
    "            if (epoch > 10):\n",
    "                self.learning_rate = 0.0001\n",
    "            if (epoch > 15):\n",
    "                self.learning_rate = 0.00001\n",
    "            total_correct = 0\n",
    "            for i in range(int(self.x_train.shape[0] / self.batch_size)):\n",
    "                self.forward_propagation()\n",
    "                prediction_train =  np.argmax(self.forward_results[\"S\"], axis=0)\n",
    "                total_correct += np.sum(prediction_train == self.y_train_sub_samples)\n",
    "                self.back_propagation()\n",
    "                self.params[\"W\"] -= self.learning_rate * self.gradients[\"dW\"]\n",
    "                self.params[\"b1\"] -= self.learning_rate * self.gradients[\"db1\"]\n",
    "                self.params[\"C\"] -= self.learning_rate * self.gradients[\"dC\"]\n",
    "                self.params[\"b2\"] -= self.learning_rate * self.gradients[\"db2\"]\n",
    "            print(\"epoch:{} | Training Accuracy:[{}]\".format(epoch+1, total_correct/len(self.x_train)))\n",
    "    def test(self):\n",
    "        self.Z = np.dot(self.params[\"W\"], self.x_test.T) + self.params[\"b1\"]\n",
    "        self.H = np.apply_along_axis(self.activation, 0, self.Z)\n",
    "        self.U = np.dot(self.params[\"C\"], self.H) + self.params[\"b2\"]\n",
    "        self.S = np.apply_along_axis(self.softmax, 0, self.U)\n",
    "        self.prediction = np.apply_along_axis(np.argmax, 0, self.S)\n",
    "        correct_ratio = np.mean(self.prediction == self.y_test)\n",
    "        return correct_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch_Size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample size: [(60000, 784)]\n",
      "test sample size:[(10000, 784)]\n",
      "hidden units number: [100]\n",
      "batch_size:[1]\n"
     ]
    }
   ],
   "source": [
    "nn = MnistModel(x_train, y_train, x_test, y_test, hidden_units=100, batch_size=1, learning_rate=0.01, num_epochs=5, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | Training Accuracy:[0.9296]\n",
      "epoch:2 | Training Accuracy:[0.96985]\n",
      "epoch:3 | Training Accuracy:[0.9783333333333334]\n",
      "epoch:4 | Training Accuracy:[0.9819166666666667]\n",
      "epoch:5 | Training Accuracy:[0.98605]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.train()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time: [672.2338092327118] second\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Time: [{}] second\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: [0.975]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: [{}]\".format(nn.test()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch_Size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample size: [(60000, 784)]\n",
      "test sample size:[(10000, 784)]\n",
      "hidden units number: [100]\n",
      "batch_size:[100]\n"
     ]
    }
   ],
   "source": [
    "nn = MnistModel(x_train, y_train, x_test, y_test, hidden_units=100, batch_size=100, learning_rate=0.01, num_epochs=5, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | Training Accuracy:[0.10268333333333333]\n",
      "epoch:2 | Training Accuracy:[0.10933333333333334]\n",
      "epoch:3 | Training Accuracy:[0.11103333333333333]\n",
      "epoch:4 | Training Accuracy:[0.11068333333333333]\n",
      "epoch:5 | Training Accuracy:[0.10808333333333334]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.train()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time: [160.43377590179443]\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Time: [{}]\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: [0.1232]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: [{}]\".format(nn.test()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `num_epochs=5` is not enough for the algorithm to converge with `batch_size=5`. Let's raise `num_epochs` to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample size: [(60000, 784)]\n",
      "test sample size:[(10000, 784)]\n",
      "hidden units number: [100]\n",
      "batch_size:[5]\n"
     ]
    }
   ],
   "source": [
    "nn = MnistModel(x_train, y_train, x_test, y_test, hidden_units=100, batch_size=5, learning_rate=0.01, num_epochs=20, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | Training Accuracy:[0.25298333333333334]\n",
      "epoch:2 | Training Accuracy:[0.28565]\n",
      "epoch:3 | Training Accuracy:[0.2926]\n",
      "epoch:4 | Training Accuracy:[0.2922]\n",
      "epoch:5 | Training Accuracy:[0.29518333333333335]\n",
      "epoch:6 | Training Accuracy:[0.29635]\n",
      "epoch:7 | Training Accuracy:[0.3075]\n",
      "epoch:8 | Training Accuracy:[0.30535]\n",
      "epoch:9 | Training Accuracy:[0.3067]\n",
      "epoch:10 | Training Accuracy:[0.30701666666666666]\n",
      "epoch:11 | Training Accuracy:[0.30565]\n",
      "epoch:12 | Training Accuracy:[0.30506666666666665]\n",
      "epoch:13 | Training Accuracy:[0.3092666666666667]\n",
      "epoch:14 | Training Accuracy:[0.3092]\n",
      "epoch:15 | Training Accuracy:[0.30625]\n",
      "epoch:16 | Training Accuracy:[0.30955]\n",
      "epoch:17 | Training Accuracy:[0.3103166666666667]\n",
      "epoch:18 | Training Accuracy:[0.30666666666666664]\n",
      "epoch:19 | Training Accuracy:[0.30816666666666664]\n",
      "epoch:20 | Training Accuracy:[0.3056833333333333]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.train()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time: [642.7738630771637]\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Time: [{}]\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: [0.168]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: [{}]\".format(nn.test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
