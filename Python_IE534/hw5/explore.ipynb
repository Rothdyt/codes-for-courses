{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import utils\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_epochs=2\n",
    "        self.batch_size=10\n",
    "        self.train_all=True\n",
    "        self.net = \"resnet18\"\n",
    "        self.resume='./hw5_checkpoint.pth.tar'\n",
    "        self.test_only=True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation...\n",
      "Loading Data...\n",
      "Model setting...\n",
      "using resnet18\n",
      "Resume from the checkpoint...\n",
      "=> loading checkpoint './hw5_checkpoint.pth.tar'\n",
      "=> loaded checkpoint './hw5_checkpoint.pth.tar' (epoch 2)\n",
      "Model Training...\n",
      "=> Trained on [2] epoch, with test accuracy [0.0].\n",
      "  During the training stages, historical best test accuracy is  [0]\n"
     ]
    }
   ],
   "source": [
    "log_level = logging.INFO\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(log_level)\n",
    "handler = logging.FileHandler(\"hw5.log\")\n",
    "handler.setLevel(log_level)\n",
    "formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.info(\"torch version: {}\".format(torch.__version__))\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "batch_size = args.batch_size\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "# Data Preparation\n",
    "\n",
    "# note that mean and std is calculated channel-wise\n",
    "# reference: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/10\n",
    "print(\"Data Preparation...\")\n",
    "logger.info(\"Data Preparation...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "logger.info(\"Loading Data...\")\n",
    "img, label = utils.generate_testing_data_set()\n",
    "test_dataset = utils.TinyImageNet(img, label, train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "if args.test_only:\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2,\n",
    "                                              sampler=SubsetRandomSampler(range(8)))\n",
    "print(\"Model setting...\")\n",
    "logger.info(\"Model setting...\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "start_epoch = 0\n",
    "if args.net == \"resnet18\":\n",
    "    print(\"using resnet18\")\n",
    "    net = torchvision.models.resnet.ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])\n",
    "    if not os.path.isfile(args.resume):\n",
    "        net.load_state_dict(torch.load(\"../data/model/resnet18-5c106cde.pth\"))\n",
    "else:\n",
    "    print(\"using resnet101\")\n",
    "    net = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 23, 3])\n",
    "    if not os.path.isfile(args.resume):\n",
    "        net.load_state_dict(torch.load(\"../data/model/resnet101-5d3b4d8f.pth\"))\n",
    "# Do not change the layers that are pre-trained with the only exception\n",
    "# on the last full-connected layer.\n",
    "if not args.train_all:\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "# change the last fc layer for cifar100\n",
    "net.fc = nn.Linear(in_features=net.fc.in_features, out_features=4096)\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    print(torch.cuda.device_count())\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06)\n",
    "training_loss_seq = []\n",
    "training_accuracy_seq = []\n",
    "testing_accuracy_seq = []\n",
    "testing_best_accuracy = 0\n",
    "\n",
    "if args.resume:\n",
    "    print(\"Resume from the checkpoint...\")\n",
    "    logger.info(\"Resume from the checkpoint...\")\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        training_loss_seq = checkpoint['training_loss_seq']\n",
    "        #training_accuracy_seq = checkpoint['training_accuracy_seq']\n",
    "        testing_accuracy_seq = checkpoint['testing_accuracy_seq']\n",
    "        testing_best_accuracy = checkpoint['testing_best_accuracy']\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args.resume, (checkpoint['epoch'] + 1)))\n",
    "        logger.info(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.resume, (checkpoint['epoch'] + 1)))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "        logger.info(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "        print(\"=> Training based on the resnet-101 from scratch...\")\n",
    "        logger.info(\"=> Training based on the resnet-101 from scratch...\")\n",
    "else:\n",
    "    print(\"=> Training based on the resnet-18 from scratch...\")\n",
    "    logger.info(\"=> Training based on the resnet-18 from scratch...\")\n",
    "\n",
    "\n",
    "print(\"Model Training...\")\n",
    "logger.info(\"Model Training...\")\n",
    "\n",
    "# use up-to-date learning rate; for resume purpose\n",
    "for param_group in optimizer.param_groups:\n",
    "    current_learningRate = param_group['lr']\n",
    "\n",
    "\n",
    "def train(epoch, k_closet=30):\n",
    "    if args.test_only:\n",
    "        k_closet = 3\n",
    "        img_triplet, label_triplet = pickle.load(open(\"./pickle/train_1.p\", 'rb'))\n",
    "        train_dataset = utils.TinyImageNet(img_triplet, label_triplet, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, num_workers=2,\n",
    "                                                   shuffle=False, sampler=SubsetRandomSampler([0, 503, 1003, 1503, 2003, 2503, 3003, 3503, 4003, 4503]))\n",
    "    else:\n",
    "        if not os.path.isfile(\"./pickle/train_{}.p\".format(epoch)):\n",
    "            img_triplet, label_triplet = utils.generate_training_data_set(save=True, epoch_idx=epoch)\n",
    "        else:\n",
    "            img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(epoch), 'rb'))\n",
    "        train_dataset = utils.TinyImageNet(img_triplet, label_triplet, train=True, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "    global current_learningRate\n",
    "    net.train()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        current_learningRate /= 2\n",
    "        logger.info(\"=> Learning rate is updated!\")\n",
    "        utils.update_learning_rate(optimizer, current_learningRate)\n",
    "\n",
    "    f_img_train = []\n",
    "    label_train = []\n",
    "    #train_bacth_accuracy = []\n",
    "    loss_epoch = 0\n",
    "    for _, (images, lables) in enumerate(train_loader):\n",
    "        #start = time.time()\n",
    "        if use_cuda:\n",
    "            q, p, n, q_label = images[0].cuda(), images[1].cuda(), images[2].cuda(), lables[0].cuda()\n",
    "        else:\n",
    "            q, p, n, q_label = images[0], images[1], images[2], lables[0]\n",
    "        optimizer.zero_grad()\n",
    "        q, p, n = Variable(q), Variable(p), Variable(n)\n",
    "        f_q, f_p, f_n = net(q), net(p), net(n)\n",
    "        loss = criterion(f_q, f_p, f_n)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.__version__ == '0.4.1':\n",
    "            loss_epoch += loss.item()\n",
    "            # print(loss_epoch)\n",
    "        else:\n",
    "            loss_epoch += loss.data[0]\n",
    "            # print(loss_epoch)\n",
    "        f_img_train.append(f_q)\n",
    "        label_train.append(q_label)\n",
    "        #end = time.time()\n",
    "        #print(\"time for one batch {} s\".format(end-start))\n",
    "        # calculate train_acc so use train_loader as the test_loader\n",
    "    #     train_accuracy = []\n",
    "    #     for f_img_test_current, label_test_current in zip(f_q, q_label):\n",
    "    #         f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "    #         f_img_test_current = f_img_test_current.expand(f_q.shape[0], 4096)\n",
    "    #         distance = pdist(f_img_test_current, f_q)\n",
    "    #         predicted = q_label[distance.topk(k_closet, largest=False)[1]]\n",
    "    #         train_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    #     train_bacth_accuracy.append(np.mean(train_accuracy))\n",
    "    # train_accuracy_epoch = np.mean(train_bacth_accuracy)\n",
    "\n",
    "    f_img_train = torch.cat(f_img_train, dim=0)\n",
    "    label_train = torch.cat(label_train, dim=0)\n",
    "\n",
    "    # train_accuracy = []\n",
    "    # # calculate train_acc so use train_loader as the test_loader\n",
    "    # for f_img_test_current, label_test_current in zip(f_img_train, label_train):\n",
    "    #     f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "    #     f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "    #     distance = pdist(f_img_test_current, f_img_train)\n",
    "    #     predicted = label_train[distance.topk(k_closet)[1]]\n",
    "    #     train_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    # train_accuracy_epoch = np.mean(train_accuracy)\n",
    "\n",
    "    # print(\"=> Epoch: [{}/{}] | Loss:[{}] | Training Accuracy: [{}]\".format(epoch + 1, args.num_epochs, loss_epoch, train_accuracy_epoch))\n",
    "    # logger.info(\"=> Epoch: [{}/{}] | Loss:[{}] | Training Accuracy: [{}]\".format(epoch + 1, args.num_epochs, loss_epoch, train_accuracy_epoch))\n",
    "    # return loss_epoch, train_accuracy_epoch, f_img_train, label_train\n",
    "    print(\"=> Epoch: [{}/{}] | Loss:[{}]\".format(epoch + 1, args.num_epochs, loss_epoch))\n",
    "    logger.info(\"=> Epoch: [{}/{}] | Loss:[{}]\".format(epoch + 1, args.num_epochs, loss_epoch))\n",
    "    return loss_epoch, f_img_train, label_train\n",
    "\n",
    "\n",
    "def test(epoch, f_img_train, label_train, k_closet=30):\n",
    "    net.eval()\n",
    "    if args.test_only:\n",
    "        k_closet = 3\n",
    "    # f_img_train = []\n",
    "    # label_train = []\n",
    "    # for _, (imgs_train, labels_train) in enumerate(train_loader):\n",
    "    #     if use_cuda:\n",
    "    #         imgs_train, labels_train = imgs_train.cuda(), labels_train.cuda()\n",
    "    #     f_img_train.append(net(imgs_train[0]))\n",
    "    #     label_train.append(labels_train[0])\n",
    "    # f_img_train = torch.cat(f_img_train, dim=0)\n",
    "    # label_train = torch.cat(label_train, dim=0)\n",
    "\n",
    "    # f_img_test = []\n",
    "    # label_test = []\n",
    "    # for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "    #     if use_cuda:\n",
    "    #         imgs_test, labels_test = imgs_test.cuda(), labels_test.cuda()\n",
    "    #     #f_img_test, label_test = Variable(f_img_test), Variable(label_test)\n",
    "    #     f_img_test.append(net(imgs_test))\n",
    "    #     label_test.append(labels_test)\n",
    "\n",
    "    # f_img_test = torch.cat(f_img_test, dim=0)\n",
    "    # label_test = torch.cat(label_test, dim=0)\n",
    "\n",
    "    # test_accuracy = []\n",
    "    # for f_img_test_current, label_test_current in zip(f_img_test, label_test):\n",
    "    #     f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "    #     f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "    #     distance = pdist(f_img_test_current, f_img_train)\n",
    "    #     predicted = label_train[distance.topk(k_closet)[1]]\n",
    "    #     test_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    # test_accuracy_epoch = np.mean(test_accuracy)\n",
    "\n",
    "    test_accuracy = []\n",
    "    for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            imgs_test, labels_test = imgs_test.cuda(), labels_test.cuda()\n",
    "        imgs_test = Variable(imgs_test)\n",
    "        with torch.no_grad():\n",
    "            f_img_test = net(imgs_test)\n",
    "        for f_img_test_current, label_test_current in zip(f_img_test, labels_test):\n",
    "            f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "            f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "            distance = pdist(f_img_test_current, f_img_train)\n",
    "            predicted = label_train[distance.topk(k_closet)[1]]\n",
    "            test_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    test_accuracy_epoch = np.mean(test_accuracy)\n",
    "\n",
    "    print(\"=> Epoch: [{}/{}] | Testing Accuracy: [{}]\".format(\n",
    "        epoch + 1, args.num_epochs, test_accuracy_epoch))\n",
    "    logger.info(\"=> Epoch: [{}/{}] | Testing Accuracy: [{}]\".format(\n",
    "        epoch + 1, args.num_epochs, test_accuracy_epoch))\n",
    "\n",
    "    return test_accuracy_epoch\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    #train_loss, train_accuracy, f_img_train, label_train = train(epoch)\n",
    "    train_loss, f_img_train, label_train = train(epoch)\n",
    "    f_img_train = f_img_train.detach()\n",
    "    # if (epoch+1) % 2 == 0:\n",
    "    test_accuracy = test(epoch, f_img_train, label_train)\n",
    "    training_loss_seq.append(train_loss)\n",
    "    # training_accuracy_seq.append(train_accuracy)\n",
    "    testing_accuracy_seq.append(test_accuracy)\n",
    "\n",
    "    is_best = testing_accuracy_seq[-1] > testing_best_accuracy\n",
    "    testing_best_accuracy = max(testing_best_accuracy, testing_accuracy_seq[-1])\n",
    "\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"state_dict\": net.state_dict(),  # if use_cuda else net.module.state_dict()\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"training_loss_seq\": training_loss_seq,\n",
    "        # \"training_accuracy_seq\": training_accuracy_seq,\n",
    "        \"testing_accuracy_seq\": testing_accuracy_seq,\n",
    "        \"testing_best_accuracy\": testing_best_accuracy\n",
    "    }\n",
    "    utils.save_checkpoint(state, is_best, filename='checkpoint.pth.tar', extra=\"hw5_\")\n",
    "    if is_best:\n",
    "        logger.info(\"=> Best parameters are updated\")\n",
    "\n",
    "\n",
    "logger.info(\"=> Trained on [{}] epoch, with test accuracy [{}].\\n \\\n",
    " During the training stages, historical best test accuracy is \\\n",
    " [{}]\".format(args.num_epochs, testing_accuracy_seq[-1], testing_best_accuracy))\n",
    "print(\"=> Trained on [{}] epoch, with test accuracy [{}].\\n \\\n",
    " During the training stages, historical best test accuracy is \\\n",
    " [{}]\".format(args.num_epochs, testing_accuracy_seq[-1], testing_best_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(2), 'rb'))\n",
    "# train_dataset = utils.TinyImageNet(img_triplet, label_triplet, train=True, transform=transform_train)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                                 batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "# f_img_train = []\n",
    "# label_train = []\n",
    "# for idx, (images, lables) in enumerate(train_loader):\n",
    "#     if idx % 100 == 0:\n",
    "#         print(idx)\n",
    "#     if use_cuda:\n",
    "#             q = images[0].cuda()\n",
    "#     else:\n",
    "#         q, q_label = images[0], lables[0]\n",
    "#     with torch.no_grad():\n",
    "#         q = Variable(q)\n",
    "#         f_q = net(q)\n",
    "#     f_img_train.append(f_q)\n",
    "#     label_train.append(q_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import utils\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_epochs=2\n",
    "        self.batch_size=10\n",
    "        self.train_all=True\n",
    "        self.net = \"resnet18\"\n",
    "        self.resume='./hw5_checkpoint.pth.tar'\n",
    "        self.test_only=True\n",
    "args = Args()\n",
    "\n",
    "net = torchvision.models.resnet.ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])\n",
    "net.fc = nn.Linear(in_features=net.fc.in_features, out_features=4096)\n",
    "train_checkpoint = torch.load(\"./train_checkpoint_{}.pth\".format(4), map_location='cpu')\n",
    "state_dict = train_checkpoint[\"state_dict\"]\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation...\n"
     ]
    }
   ],
   "source": [
    "batch_size = args.batch_size\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "# Data Preparation\n",
    "\n",
    "# note that mean and std is calculated channel-wise\n",
    "# reference: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/10\n",
    "print(\"Data Preparation...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(2), 'rb'))\n",
    "train_dataset = utils.TinyImageNet(img_triplet, label_triplet, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, num_workers=2,\n",
    "                                           shuffle=False, sampler=SubsetRandomSampler(range(5000))) \n",
    "# [0, 1,2,3,\n",
    "# 503, 1003, 1503, \n",
    "# 2003, 2004, 2005 ,\n",
    "# 2503, 3003, 3503, 4003, 4503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = utils.generate_testing_data_set()\n",
    "test_dataset = utils.TinyImageNet(img, label, train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=2,\n",
    "                                              sampler=SubsetRandomSampler([23, 68, 50, 70, 77, 249]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, train_loader, k_closet=30, use_cuda=False):\n",
    "    net.eval()\n",
    "    if args.test_only:\n",
    "        k_closet = 3\n",
    "    f_img_train = []\n",
    "    label_train = []\n",
    "    test_accuracy = []\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        print(\"Calculate Training Feature Embedding...\")\n",
    "        for _, (images, lables) in enumerate(train_loader):\n",
    "            if use_cuda:\n",
    "                q, q_label = images[0].cuda(), lables[0].cuda()\n",
    "            else:\n",
    "                q, q_label = images[0], lables[0]\n",
    "            q = Variable(q)\n",
    "            f_q = net(q)\n",
    "            f_img_train.append(f_q)\n",
    "            label_train.append(q_label)\n",
    "        f_img_train = torch.cat(f_img_train, dim=0)\n",
    "        label_train = torch.cat(label_train, dim=0)\n",
    "        end = time.time()\n",
    "        print(\"Finish in {} min\".format((end-start)/60))\n",
    "        print(\"Testing...\")\n",
    "        for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                imgs_test, labels_test = imgs_test.cuda(), labels_test.cuda()\n",
    "            imgs_test = Variable(imgs_test)\n",
    "            f_img_test = net(imgs_test)\n",
    "            for f_img_test_current, label_test_current in zip(f_img_test, labels_test):\n",
    "                f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "                f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "                distance = pdist(f_img_test_current, f_img_train)\n",
    "                predicted = label_train[distance.topk(k_closet, largest=False)[1]]\n",
    "                test_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "        test_accuracy_epoch = np.mean(test_accuracy)\n",
    "\n",
    "        print(\"=> Epoch: [{}/{}] | Testing Accuracy: [{}]\".format(\n",
    "            epoch + 1, args.num_epochs, test_accuracy_epoch))\n",
    "    return test_accuracy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Training Feature Embedding...\n",
      "Finish in 7.589399047692617 min\n",
      "Testing...\n",
      "=> Epoch: [1/2] | Testing Accuracy: [0.4444444444444444]\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(0, train_loader, k_closet=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /Users/ym/.torch/models/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import utils\n",
    "net = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3])\n",
    "net.fc = nn.Linear(in_features=net.fc.in_features, out_features=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = torch.load(\"./checkpoint.pth.tar\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = check[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[13:] # remove `module.`\n",
    "    new_state_dict[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = utils.generate_testing_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "img, label = utils.generate_testing_data_set()\n",
    "val_img, val_label = img[0:4], label[0:4]\n",
    "test_dataset = utils.TinyImageNet(val_img, val_label, train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=32)\n",
    "# f_img_train, label_train = pickle.load(open(\"./pickle/resnet50_embedding_{}.p\".format(epoch), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 3, 224, 224]) tensor([107])\n",
      "1 torch.Size([1, 3, 224, 224]) tensor([139])\n",
      "2 torch.Size([1, 3, 224, 224]) tensor([140])\n",
      "3 torch.Size([1, 3, 224, 224]) tensor([69])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(0), 'rb'))\n",
    "train_dataset = utils.TinyImageNet(img_triplet, label_triplet, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "1 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "2 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "3 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "4 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "5 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "6 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "7 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "8 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "9 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "10 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (img, label) in enumerate(train_loader):\n",
    "    q, q_label = img[0], label[0]\n",
    "    if idx <= 10:\n",
    "        print(idx, q.shape, q_label)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "train_dict, db = utils.create_database()\n",
    "img_all = [db[i] for i in db.keys()]\n",
    "img_all = [item for l in img_all for item in l]\n",
    "img, label = utils.generate_testing_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "top10, bottom10 = torch.load(\"./final.pth\",map_location='cpu')\n",
    "q_num = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = top10[\"idx_of_fig_path_top10\"][q_num].numpy()\n",
    "path = [img_all[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/tiny-imagenet-200/val/images/val_2.JPEG', 140)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[2], label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAg70lEQVR4nFV6Z+xt2XXXb61dTrvn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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x118E52F28>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.open(img[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAbl0lEQVR4nH16e5BmR3XfOd19n9/7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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x1199874E0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(path[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distance_of_top10': [tensor([6.5760, 6.9267, 7.2467, 7.6946, 7.7326, 7.7482, 7.7864, 7.8396, 7.8859,\n",
       "          7.8863]),\n",
       "  tensor([7.9602, 8.3556, 8.4161, 8.5197, 8.5304, 8.6306, 8.6322, 8.6342, 8.6908,\n",
       "          8.7120]),\n",
       "  tensor([5.8009, 6.0161, 6.1197, 6.1806, 6.2166, 6.2326, 6.3146, 6.3815, 6.3844,\n",
       "          6.3962]),\n",
       "  tensor([7.7558, 7.9154, 8.3444, 8.3492, 8.3644, 8.3922, 8.4416, 8.4494, 8.4824,\n",
       "          8.4845]),\n",
       "  tensor([6.9445, 6.9890, 7.2388, 7.2870, 7.3191, 7.4607, 7.4867, 7.4954, 7.5284,\n",
       "          7.6040])],\n",
       " 'predicted_top10': [tensor([153, 153, 153, 153, 153, 148, 153, 153, 153, 153]),\n",
       "  tensor([170, 187,   2, 171, 106, 183, 147, 183, 147, 183]),\n",
       "  tensor([174, 174, 174, 103, 174, 174, 103, 144, 174, 174]),\n",
       "  tensor([158,  19,  96, 164,  19, 195, 195, 164,  85, 195]),\n",
       "  tensor([ 49, 190, 190, 190, 190, 115,  41, 190, 190, 190])],\n",
       " 'idx_of_fig_path_top10': [tensor([76760, 76684, 76532, 76802, 76699, 74470, 76505, 76621, 76844, 76781]),\n",
       "  tensor([85433, 93871,  1260, 85982, 53053, 91536, 73683, 91636, 73561, 91842]),\n",
       "  tensor([87384, 87071, 87152, 51542, 87423, 87233, 51737, 72073, 87330, 87142]),\n",
       "  tensor([79318,  9814, 48276, 82211,  9832, 97581, 97593, 82177, 42772, 97667]),\n",
       "  tensor([24788, 95254, 95209, 95012, 95280, 57542, 20708, 95051, 95148, 95418])]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytop10 = {\"path\":[], \"distance\":[], \"label\":[]}\n",
    "count = 0\n",
    "for j in [0,21,674, 975, 2001]:\n",
    "    query_fig_idx = label[j]\n",
    "    n_label = [i for i in train_dict.keys()][query_fig_idx]\n",
    "    mytop10[\"path\"].append(db[n_label][:11])\n",
    "    mytop10[\"distance\"].append(top10['distance_of_top10'][count])\n",
    "    mytop10[\"label\"].append([n_label]*10)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybottom10 = {\"path\":[], \"distance\":[], \"label\":[]}\n",
    "count = 0\n",
    "for j in [0,21,674, 975, 2001]:\n",
    "    #idx = bottom10[\"idx_of_fig_path_bottom10\"][count].numpy()\n",
    "    path = [img_all[i] for i in np.random.choice(range(100000), 10)]\n",
    "    mybottom10[\"path\"].append(path)\n",
    "    mybottom10[\"distance\"].append(bottom10['distance_of_bottom10'][count])\n",
    "    mybottom10[\"label\"].append([path[i].split(\"/\")[4] for i in range(10)])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = {\"path\":[], \"label\":[]}\n",
    "for j in [0,21,674, 975, 2001]:\n",
    "    query_fig_idx = label[j]\n",
    "    n_label = [i for i in train_dict.keys()][query_fig_idx]\n",
    "    query_list[\"path\"].append(img[j])\n",
    "    query_list[\"label\"].append([n_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((query_list, mytop10, mybottom10, loss), file=open(\"./myfinal.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [0.023586829798318505, 0.013265381048618964, 0.011313620450353156 ,0.010403190940040876,0.008552217199533916,0.007924619082339705,0.007454473718487061,0.0070725912862916445, 0.006998448625764722, 0.006250378639476185, 0.005731241940571726, 0.005593074555785206,0.0054566194354433535, 0.005245383240342053,0.005068134553935561]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
