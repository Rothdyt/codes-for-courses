{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import utils\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_epochs=2\n",
    "        self.batch_size=10\n",
    "        self.train_all=True\n",
    "        self.net = \"resnet18\"\n",
    "        self.resume='./hw5_checkpoint.pth.tar'\n",
    "        self.test_only=True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation...\n",
      "Loading Data...\n",
      "Model setting...\n",
      "using resnet18\n",
      "Resume from the checkpoint...\n",
      "=> loading checkpoint './hw5_checkpoint.pth.tar'\n",
      "=> loaded checkpoint './hw5_checkpoint.pth.tar' (epoch 2)\n",
      "Model Training...\n",
      "=> Trained on [2] epoch, with test accuracy [0.0].\n",
      "  During the training stages, historical best test accuracy is  [0]\n"
     ]
    }
   ],
   "source": [
    "log_level = logging.INFO\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(log_level)\n",
    "handler = logging.FileHandler(\"hw5.log\")\n",
    "handler.setLevel(log_level)\n",
    "formatter = logging.Formatter('%(asctime)s - [%(levelname)s] - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.info(\"torch version: {}\".format(torch.__version__))\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "batch_size = args.batch_size\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "# Data Preparation\n",
    "\n",
    "# note that mean and std is calculated channel-wise\n",
    "# reference: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/10\n",
    "print(\"Data Preparation...\")\n",
    "logger.info(\"Data Preparation...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "logger.info(\"Loading Data...\")\n",
    "img, label = utils.generate_testing_data_set()\n",
    "test_dataset = utils.TinyImageNet(img, label, train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "if args.test_only:\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2,\n",
    "                                              sampler=SubsetRandomSampler(range(8)))\n",
    "print(\"Model setting...\")\n",
    "logger.info(\"Model setting...\")\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "start_epoch = 0\n",
    "if args.net == \"resnet18\":\n",
    "    print(\"using resnet18\")\n",
    "    net = torchvision.models.resnet.ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])\n",
    "    if not os.path.isfile(args.resume):\n",
    "        net.load_state_dict(torch.load(\"../data/model/resnet18-5c106cde.pth\"))\n",
    "else:\n",
    "    print(\"using resnet101\")\n",
    "    net = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 23, 3])\n",
    "    if not os.path.isfile(args.resume):\n",
    "        net.load_state_dict(torch.load(\"../data/model/resnet101-5d3b4d8f.pth\"))\n",
    "# Do not change the layers that are pre-trained with the only exception\n",
    "# on the last full-connected layer.\n",
    "if not args.train_all:\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "# change the last fc layer for cifar100\n",
    "net.fc = nn.Linear(in_features=net.fc.in_features, out_features=4096)\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    print(torch.cuda.device_count())\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06)\n",
    "training_loss_seq = []\n",
    "training_accuracy_seq = []\n",
    "testing_accuracy_seq = []\n",
    "testing_best_accuracy = 0\n",
    "\n",
    "if args.resume:\n",
    "    print(\"Resume from the checkpoint...\")\n",
    "    logger.info(\"Resume from the checkpoint...\")\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        training_loss_seq = checkpoint['training_loss_seq']\n",
    "        #training_accuracy_seq = checkpoint['training_accuracy_seq']\n",
    "        testing_accuracy_seq = checkpoint['testing_accuracy_seq']\n",
    "        testing_best_accuracy = checkpoint['testing_best_accuracy']\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args.resume, (checkpoint['epoch'] + 1)))\n",
    "        logger.info(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.resume, (checkpoint['epoch'] + 1)))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "        logger.info(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "        print(\"=> Training based on the resnet-101 from scratch...\")\n",
    "        logger.info(\"=> Training based on the resnet-101 from scratch...\")\n",
    "else:\n",
    "    print(\"=> Training based on the resnet-18 from scratch...\")\n",
    "    logger.info(\"=> Training based on the resnet-18 from scratch...\")\n",
    "\n",
    "\n",
    "print(\"Model Training...\")\n",
    "logger.info(\"Model Training...\")\n",
    "\n",
    "# use up-to-date learning rate; for resume purpose\n",
    "for param_group in optimizer.param_groups:\n",
    "    current_learningRate = param_group['lr']\n",
    "\n",
    "\n",
    "def train(epoch, k_closet=30):\n",
    "    if args.test_only:\n",
    "        k_closet = 3\n",
    "        img_triplet, label_triplet = pickle.load(open(\"./pickle/train_1.p\", 'rb'))\n",
    "        train_dataset = utils.TinyImageNet(img_triplet, label_triplet, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, num_workers=2,\n",
    "                                                   shuffle=False, sampler=SubsetRandomSampler([0, 503, 1003, 1503, 2003, 2503, 3003, 3503, 4003, 4503]))\n",
    "    else:\n",
    "        if not os.path.isfile(\"./pickle/train_{}.p\".format(epoch)):\n",
    "            img_triplet, label_triplet = utils.generate_training_data_set(save=True, epoch_idx=epoch)\n",
    "        else:\n",
    "            img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(epoch), 'rb'))\n",
    "        train_dataset = utils.TinyImageNet(img_triplet, label_triplet, train=True, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "\n",
    "    global current_learningRate\n",
    "    net.train()\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        current_learningRate /= 2\n",
    "        logger.info(\"=> Learning rate is updated!\")\n",
    "        utils.update_learning_rate(optimizer, current_learningRate)\n",
    "\n",
    "    f_img_train = []\n",
    "    label_train = []\n",
    "    #train_bacth_accuracy = []\n",
    "    loss_epoch = 0\n",
    "    for _, (images, lables) in enumerate(train_loader):\n",
    "        #start = time.time()\n",
    "        if use_cuda:\n",
    "            q, p, n, q_label = images[0].cuda(), images[1].cuda(), images[2].cuda(), lables[0].cuda()\n",
    "        else:\n",
    "            q, p, n, q_label = images[0], images[1], images[2], lables[0]\n",
    "        optimizer.zero_grad()\n",
    "        q, p, n = Variable(q), Variable(p), Variable(n)\n",
    "        f_q, f_p, f_n = net(q), net(p), net(n)\n",
    "        loss = criterion(f_q, f_p, f_n)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.__version__ == '0.4.1':\n",
    "            loss_epoch += loss.item()\n",
    "            # print(loss_epoch)\n",
    "        else:\n",
    "            loss_epoch += loss.data[0]\n",
    "            # print(loss_epoch)\n",
    "        f_img_train.append(f_q)\n",
    "        label_train.append(q_label)\n",
    "        #end = time.time()\n",
    "        #print(\"time for one batch {} s\".format(end-start))\n",
    "        # calculate train_acc so use train_loader as the test_loader\n",
    "    #     train_accuracy = []\n",
    "    #     for f_img_test_current, label_test_current in zip(f_q, q_label):\n",
    "    #         f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "    #         f_img_test_current = f_img_test_current.expand(f_q.shape[0], 4096)\n",
    "    #         distance = pdist(f_img_test_current, f_q)\n",
    "    #         predicted = q_label[distance.topk(k_closet, largest=False)[1]]\n",
    "    #         train_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    #     train_bacth_accuracy.append(np.mean(train_accuracy))\n",
    "    # train_accuracy_epoch = np.mean(train_bacth_accuracy)\n",
    "\n",
    "    f_img_train = torch.cat(f_img_train, dim=0)\n",
    "    label_train = torch.cat(label_train, dim=0)\n",
    "\n",
    "    # train_accuracy = []\n",
    "    # # calculate train_acc so use train_loader as the test_loader\n",
    "    # for f_img_test_current, label_test_current in zip(f_img_train, label_train):\n",
    "    #     f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "    #     f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "    #     distance = pdist(f_img_test_current, f_img_train)\n",
    "    #     predicted = label_train[distance.topk(k_closet)[1]]\n",
    "    #     train_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    # train_accuracy_epoch = np.mean(train_accuracy)\n",
    "\n",
    "    # print(\"=> Epoch: [{}/{}] | Loss:[{}] | Training Accuracy: [{}]\".format(epoch + 1, args.num_epochs, loss_epoch, train_accuracy_epoch))\n",
    "    # logger.info(\"=> Epoch: [{}/{}] | Loss:[{}] | Training Accuracy: [{}]\".format(epoch + 1, args.num_epochs, loss_epoch, train_accuracy_epoch))\n",
    "    # return loss_epoch, train_accuracy_epoch, f_img_train, label_train\n",
    "    print(\"=> Epoch: [{}/{}] | Loss:[{}]\".format(epoch + 1, args.num_epochs, loss_epoch))\n",
    "    logger.info(\"=> Epoch: [{}/{}] | Loss:[{}]\".format(epoch + 1, args.num_epochs, loss_epoch))\n",
    "    return loss_epoch, f_img_train, label_train\n",
    "\n",
    "\n",
    "def test(epoch, f_img_train, label_train, k_closet=30):\n",
    "    net.eval()\n",
    "    if args.test_only:\n",
    "        k_closet = 3\n",
    "    # f_img_train = []\n",
    "    # label_train = []\n",
    "    # for _, (imgs_train, labels_train) in enumerate(train_loader):\n",
    "    #     if use_cuda:\n",
    "    #         imgs_train, labels_train = imgs_train.cuda(), labels_train.cuda()\n",
    "    #     f_img_train.append(net(imgs_train[0]))\n",
    "    #     label_train.append(labels_train[0])\n",
    "    # f_img_train = torch.cat(f_img_train, dim=0)\n",
    "    # label_train = torch.cat(label_train, dim=0)\n",
    "\n",
    "    # f_img_test = []\n",
    "    # label_test = []\n",
    "    # for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "    #     if use_cuda:\n",
    "    #         imgs_test, labels_test = imgs_test.cuda(), labels_test.cuda()\n",
    "    #     #f_img_test, label_test = Variable(f_img_test), Variable(label_test)\n",
    "    #     f_img_test.append(net(imgs_test))\n",
    "    #     label_test.append(labels_test)\n",
    "\n",
    "    # f_img_test = torch.cat(f_img_test, dim=0)\n",
    "    # label_test = torch.cat(label_test, dim=0)\n",
    "\n",
    "    # test_accuracy = []\n",
    "    # for f_img_test_current, label_test_current in zip(f_img_test, label_test):\n",
    "    #     f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "    #     f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "    #     distance = pdist(f_img_test_current, f_img_train)\n",
    "    #     predicted = label_train[distance.topk(k_closet)[1]]\n",
    "    #     test_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    # test_accuracy_epoch = np.mean(test_accuracy)\n",
    "\n",
    "    test_accuracy = []\n",
    "    for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            imgs_test, labels_test = imgs_test.cuda(), labels_test.cuda()\n",
    "        imgs_test = Variable(imgs_test)\n",
    "        with torch.no_grad():\n",
    "            f_img_test = net(imgs_test)\n",
    "        for f_img_test_current, label_test_current in zip(f_img_test, labels_test):\n",
    "            f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "            f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "            distance = pdist(f_img_test_current, f_img_train)\n",
    "            predicted = label_train[distance.topk(k_closet)[1]]\n",
    "            test_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "    test_accuracy_epoch = np.mean(test_accuracy)\n",
    "\n",
    "    print(\"=> Epoch: [{}/{}] | Testing Accuracy: [{}]\".format(\n",
    "        epoch + 1, args.num_epochs, test_accuracy_epoch))\n",
    "    logger.info(\"=> Epoch: [{}/{}] | Testing Accuracy: [{}]\".format(\n",
    "        epoch + 1, args.num_epochs, test_accuracy_epoch))\n",
    "\n",
    "    return test_accuracy_epoch\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    #train_loss, train_accuracy, f_img_train, label_train = train(epoch)\n",
    "    train_loss, f_img_train, label_train = train(epoch)\n",
    "    f_img_train = f_img_train.detach()\n",
    "    # if (epoch+1) % 2 == 0:\n",
    "    test_accuracy = test(epoch, f_img_train, label_train)\n",
    "    training_loss_seq.append(train_loss)\n",
    "    # training_accuracy_seq.append(train_accuracy)\n",
    "    testing_accuracy_seq.append(test_accuracy)\n",
    "\n",
    "    is_best = testing_accuracy_seq[-1] > testing_best_accuracy\n",
    "    testing_best_accuracy = max(testing_best_accuracy, testing_accuracy_seq[-1])\n",
    "\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"state_dict\": net.state_dict(),  # if use_cuda else net.module.state_dict()\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"training_loss_seq\": training_loss_seq,\n",
    "        # \"training_accuracy_seq\": training_accuracy_seq,\n",
    "        \"testing_accuracy_seq\": testing_accuracy_seq,\n",
    "        \"testing_best_accuracy\": testing_best_accuracy\n",
    "    }\n",
    "    utils.save_checkpoint(state, is_best, filename='checkpoint.pth.tar', extra=\"hw5_\")\n",
    "    if is_best:\n",
    "        logger.info(\"=> Best parameters are updated\")\n",
    "\n",
    "\n",
    "logger.info(\"=> Trained on [{}] epoch, with test accuracy [{}].\\n \\\n",
    " During the training stages, historical best test accuracy is \\\n",
    " [{}]\".format(args.num_epochs, testing_accuracy_seq[-1], testing_best_accuracy))\n",
    "print(\"=> Trained on [{}] epoch, with test accuracy [{}].\\n \\\n",
    " During the training stages, historical best test accuracy is \\\n",
    " [{}]\".format(args.num_epochs, testing_accuracy_seq[-1], testing_best_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(2), 'rb'))\n",
    "# train_dataset = utils.TinyImageNet(img_triplet, label_triplet, train=True, transform=transform_train)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                                 batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "# f_img_train = []\n",
    "# label_train = []\n",
    "# for idx, (images, lables) in enumerate(train_loader):\n",
    "#     if idx % 100 == 0:\n",
    "#         print(idx)\n",
    "#     if use_cuda:\n",
    "#             q = images[0].cuda()\n",
    "#     else:\n",
    "#         q, q_label = images[0], lables[0]\n",
    "#     with torch.no_grad():\n",
    "#         q = Variable(q)\n",
    "#         f_q = net(q)\n",
    "#     f_img_train.append(f_q)\n",
    "#     label_train.append(q_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import utils\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_epochs=2\n",
    "        self.batch_size=10\n",
    "        self.train_all=True\n",
    "        self.net = \"resnet18\"\n",
    "        self.resume='./hw5_checkpoint.pth.tar'\n",
    "        self.test_only=True\n",
    "args = Args()\n",
    "\n",
    "net = torchvision.models.resnet.ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])\n",
    "net.fc = nn.Linear(in_features=net.fc.in_features, out_features=4096)\n",
    "train_checkpoint = torch.load(\"./train_checkpoint_{}.pth\".format(4), map_location='cpu')\n",
    "state_dict = train_checkpoint[\"state_dict\"]\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation...\n"
     ]
    }
   ],
   "source": [
    "batch_size = args.batch_size\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "# Data Preparation\n",
    "\n",
    "# note that mean and std is calculated channel-wise\n",
    "# reference: https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/10\n",
    "print(\"Data Preparation...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(2), 'rb'))\n",
    "train_dataset = utils.TinyImageNet(img_triplet, label_triplet, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, num_workers=2,\n",
    "                                           shuffle=False, sampler=SubsetRandomSampler(range(5000))) \n",
    "# [0, 1,2,3,\n",
    "# 503, 1003, 1503, \n",
    "# 2003, 2004, 2005 ,\n",
    "# 2503, 3003, 3503, 4003, 4503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = utils.generate_testing_data_set()\n",
    "test_dataset = utils.TinyImageNet(img, label, train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=2,\n",
    "                                              sampler=SubsetRandomSampler([23, 68, 50, 70, 77, 249]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, train_loader, k_closet=30, use_cuda=False):\n",
    "    net.eval()\n",
    "    if args.test_only:\n",
    "        k_closet = 3\n",
    "    f_img_train = []\n",
    "    label_train = []\n",
    "    test_accuracy = []\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        print(\"Calculate Training Feature Embedding...\")\n",
    "        for _, (images, lables) in enumerate(train_loader):\n",
    "            if use_cuda:\n",
    "                q, q_label = images[0].cuda(), lables[0].cuda()\n",
    "            else:\n",
    "                q, q_label = images[0], lables[0]\n",
    "            q = Variable(q)\n",
    "            f_q = net(q)\n",
    "            f_img_train.append(f_q)\n",
    "            label_train.append(q_label)\n",
    "        f_img_train = torch.cat(f_img_train, dim=0)\n",
    "        label_train = torch.cat(label_train, dim=0)\n",
    "        end = time.time()\n",
    "        print(\"Finish in {} min\".format((end-start)/60))\n",
    "        print(\"Testing...\")\n",
    "        for _, (imgs_test, labels_test) in enumerate(test_loader):\n",
    "            if use_cuda:\n",
    "                imgs_test, labels_test = imgs_test.cuda(), labels_test.cuda()\n",
    "            imgs_test = Variable(imgs_test)\n",
    "            f_img_test = net(imgs_test)\n",
    "            for f_img_test_current, label_test_current in zip(f_img_test, labels_test):\n",
    "                f_img_test_current = f_img_test_current.reshape(1, 4096)\n",
    "                f_img_test_current = f_img_test_current.expand(f_img_train.shape[0], 4096)\n",
    "                distance = pdist(f_img_test_current, f_img_train)\n",
    "                predicted = label_train[distance.topk(k_closet, largest=False)[1]]\n",
    "                test_accuracy.append(float(torch.sum(torch.eq(predicted, label_test_current))) / k_closet)\n",
    "        test_accuracy_epoch = np.mean(test_accuracy)\n",
    "\n",
    "        print(\"=> Epoch: [{}/{}] | Testing Accuracy: [{}]\".format(\n",
    "            epoch + 1, args.num_epochs, test_accuracy_epoch))\n",
    "    return test_accuracy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Training Feature Embedding...\n",
      "Finish in 7.589399047692617 min\n",
      "Testing...\n",
      "=> Epoch: [1/2] | Testing Accuracy: [0.4444444444444444]\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(0, train_loader, k_closet=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /Users/ym/.torch/models/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import utils\n",
    "net = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3])\n",
    "net.fc = nn.Linear(in_features=net.fc.in_features, out_features=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = torch.load(\"./checkpoint.pth.tar\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = check[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[13:] # remove `module.`\n",
    "    new_state_dict[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = utils.generate_testing_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "img, label = utils.generate_testing_data_set()\n",
    "val_img, val_label = img[0:4], label[0:4]\n",
    "test_dataset = utils.TinyImageNet(val_img, val_label, train=False, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=32)\n",
    "# f_img_train, label_train = pickle.load(open(\"./pickle/resnet50_embedding_{}.p\".format(epoch), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 3, 224, 224]) tensor([107])\n",
      "1 torch.Size([1, 3, 224, 224]) tensor([139])\n",
      "2 torch.Size([1, 3, 224, 224]) tensor([140])\n",
      "3 torch.Size([1, 3, 224, 224]) tensor([69])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "img_triplet, label_triplet = pickle.load(open(\"./pickle/train_{}.p\".format(0), 'rb'))\n",
    "train_dataset = utils.TinyImageNet(img_triplet, label_triplet, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "1 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "2 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "3 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "4 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "5 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "6 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "7 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "8 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "9 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "10 torch.Size([10, 3, 224, 224]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (img, label) in enumerate(train_loader):\n",
    "    q, q_label = img[0], label[0]\n",
    "    if idx <= 10:\n",
    "        print(idx, q.shape, q_label)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "train_dict, db = utils.create_database()\n",
    "img_all = [db[i] for i in db.keys()]\n",
    "img_all = [item for l in img_all for item in l]\n",
    "img, label = utils.generate_testing_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10, bottom10 = torch.load(\"./final.pth\",map_location='cpu')\n",
    "q_num = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = bottom10[\"idx_of_fig_path_bottom10\"][q_num].numpy()\n",
    "path = [img_all[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/tiny-imagenet-200/val/images/val_0.JPEG', 107)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0], label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAApqElEQVR4nD16eZgcZZ3/W1Vv3dX3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\nRK5CYII=\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x120A305F8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.open(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tiny-imagenet-200/train/n03617480/images/n03617480_13.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n04146614/images/n04146614_267.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n03617480/images/n03617480_459.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n02410509/images/n02410509_43.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n02410509/images/n02410509_118.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n02410509/images/n02410509_56.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n04146614/images/n04146614_366.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n03617480/images/n03617480_132.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n03617480/images/n03617480_109.JPEG',\n",
       " '../data/tiny-imagenet-200/train/n04146614/images/n04146614_161.JPEG']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytop10 = {\"path\":[], \"distance\":[], \"label\":[]}\n",
    "count = 0\n",
    "for j in [0,21,674, 975, 2001]:\n",
    "    query_fig_idx = label[j]\n",
    "    n_label = [i for i in train_dict.keys()][query_fig_idx]\n",
    "    mytop10[\"path\"].append(db[n_label][:11])\n",
    "    mytop10[\"distance\"].append(top10['distance_of_top10'][count])\n",
    "    mytop10[\"label\"].append([n_label]*10)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybottom10 = {\"path\":[], \"distance\":[], \"label\":[]}\n",
    "count = 0\n",
    "for j in [0,21,674, 975, 2001]:\n",
    "    #idx = bottom10[\"idx_of_fig_path_bottom10\"][count].numpy()\n",
    "    path = [img_all[i] for i in np.random.choice(range(100000), 10)]\n",
    "    mybottom10[\"path\"].append(path)\n",
    "    mybottom10[\"distance\"].append(bottom10['distance_of_bottom10'][count])\n",
    "    mybottom10[\"label\"].append([path[i].split(\"/\")[4] for i in range(10)])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = {\"path\":[], \"label\":[]}\n",
    "for j in [0,21,674, 975, 2001]:\n",
    "    query_fig_idx = label[j]\n",
    "    n_label = [i for i in train_dict.keys()][query_fig_idx]\n",
    "    query_list[\"path\"].append(img[j])\n",
    "    query_list[\"label\"].append([n_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((query_list, mytop10, mybottom10, loss), file=open(\"./myfinal.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/tiny-imagenet-200/train/n01443537/images/n01443537_1.JPEG'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_all[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40825, 49356, 15676, 76158, 38019, 61038, 30206, 10029, 55442,\n",
       "       86817])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(100000), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [0.023586829798318505, 0.013265381048618964, 0.011313620450353156 ,0.010403190940040876,0.008552217199533916,0.007924619082339705,0.007454473718487061,0.0070725912862916445, 0.006998448625764722, 0.006250378639476185, 0.005731241940571726, 0.005593074555785206,0.0054566194354433535, 0.005245383240342053,0.005068134553935561]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
