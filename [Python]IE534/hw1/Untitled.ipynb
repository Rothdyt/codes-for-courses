{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time \n",
    "import copy\n",
    "#import logging\n",
    "#from helperfunctions import create_log\n",
    "#logger = create_log(file_name=\"task.log\", log_level=logging.DEBUG)\n",
    "file_name = \"../data/MNISTdata.hdf5\"\n",
    "#logger.info(\"Load the MNIST dataset...\")\n",
    "data = h5py.File(file_name, \"r\")\n",
    "x_train = np.float32(data[\"x_train\"][:])\n",
    "y_train = np.int32(np.hstack(np.array(data[\"y_train\"])))\n",
    "x_test = np.float32(data[\"x_test\"][:])\n",
    "y_test = np.int32(np.hstack(np.array(data[\"y_test\"])))\n",
    "data.close()\n",
    "#logger.info(\"Finished!\")\n",
    "class MnistModel():\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, hidden_units=100, learning_rate=0.01, batch_size=20, num_epochs=5, seed=None):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.num_inputs = self.x_train.shape[1]\n",
    "        self.num_outputs = 10\n",
    "        self.hidden_units = hidden_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.params = {}\n",
    "        self.gradients = {}\n",
    "        if seed is not None:\n",
    "            r = np.random.RandomState(seed)\n",
    "            self.params[\"W\"] = r.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))  \n",
    "            self.params[\"C\"] = r.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1)) \n",
    "        else:\n",
    "            self.params[\"W\"] = np.random.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))\n",
    "            self.params[\"C\"] = np.random.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1))\n",
    "        print(\"training sample size: [{}]\\ntest sample size:[{}]\\nhidden units number: [{}]\\nbatch_size:[{}]\".format(self.x_train.shape, self.x_test.shape, self.hidden_units, self.batch_size))\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: x if x > 0 else 0, z)]\n",
    "\n",
    "    def activation_gradient(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: 1 if x > 0 else 0, z)]\n",
    "\n",
    "    def softmax(self, U):\n",
    "        temp = np.exp(U)\n",
    "        return temp / np.sum(temp)\n",
    "\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        random_index = np.random.choice(self.x_train.shape[0], replace=False, size=self.batch_size)\n",
    "        self.x_train_sub_samples = self.x_train[random_index].reshape((-1, self.batch_size))\n",
    "        self.y_train_sub_samples = self.y_train[random_index]\n",
    "        self.forward_results = {}\n",
    "        self.forward_results[\"Z\"] = np.dot(self.params[\"W\"], self.x_train_sub_samples) + self.params[\"b1\"]\n",
    "        self.forward_results[\"H\"] = np.apply_along_axis(self.activation, 0, self.forward_results[\"Z\"])\n",
    "        self.forward_results[\"U\"] = np.dot(self.params[\"C\"], self.forward_results[\"H\"]) + self.params[\"b2\"]\n",
    "        self.forward_results[\"S\"] = np.apply_along_axis(self.softmax, 0, self.forward_results[\"U\"])\n",
    "\n",
    "    def create_unit_matrix(self):\n",
    "        ey = np.zeros((self.num_outputs, self.batch_size))\n",
    "        for col_index, row_index in enumerate(self.y_train_sub_samples):\n",
    "            ey[row_index, col_index] = 1\n",
    "        return(ey)\n",
    "\n",
    "    def back_propagation(self):\n",
    "        ey = self.create_unit_matrix()\n",
    "        temp = - (ey - self.forward_results[\"S\"])\n",
    "        self.gradients[\"db2\"] = np.mean(temp, axis=1, keepdims=True)\n",
    "        self.gradients[\"dC\"] = np.dot(temp, self.forward_results[\"H\"].T) / self.batch_size\n",
    "        self.gradients[\"dH\"] = np.mean(np.dot(self.params[\"C\"].T, temp), axis=1, keepdims=True)\n",
    "        H_gradient = np.apply_along_axis(self.activation_gradient, 0, self.forward_results[\"Z\"])\n",
    "        temp2 = np.multiply(self.gradients[\"dH\"], H_gradient)\n",
    "        self.gradients[\"db1\"] = np.mean(temp2, axis=1, keepdims=True)\n",
    "        self.gradients[\"dW\"] = np.dot(temp2, self.x_train_sub_samples.T) / self.batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if (epoch > 5):\n",
    "                self.learning_rate = 0.001\n",
    "            if (epoch > 10):\n",
    "                self.learning_rate = 0.0001\n",
    "            if (epoch > 15):\n",
    "                self.learning_rate = 0.00001\n",
    "            total_correct = 0\n",
    "            for i in range(int(self.x_train.shape[0] / self.batch_size)):\n",
    "                self.forward_propagation()\n",
    "                prediction_train =  np.argmax(self.forward_results[\"S\"], axis=0)\n",
    "                total_correct += np.sum(prediction_train == self.y_train_sub_samples)\n",
    "                self.back_propagation()\n",
    "                self.params[\"W\"] -= self.learning_rate * self.gradients[\"dW\"]\n",
    "                self.params[\"b1\"] -= self.learning_rate * self.gradients[\"db1\"]\n",
    "                self.params[\"C\"] -= self.learning_rate * self.gradients[\"dC\"]\n",
    "                self.params[\"b2\"] -= self.learning_rate * self.gradients[\"db2\"]\n",
    "            print(\"epoch:{} | Training Accuracy:[{}]\".format(epoch+1, total_correct/len(self.x_train)))\n",
    "    def test(self):\n",
    "        self.Z = np.dot(self.params[\"W\"], self.x_test.T) + self.params[\"b1\"]\n",
    "        self.H = np.apply_along_axis(self.activation, 0, self.Z)\n",
    "        self.U = np.dot(self.params[\"C\"], self.H) + self.params[\"b2\"]\n",
    "        self.S = np.apply_along_axis(self.softmax, 0, self.U)\n",
    "        self.prediction = np.apply_along_axis(np.argmax, 0, self.S)\n",
    "        correct_ratio = np.mean(self.prediction == self.y_test)\n",
    "        return correct_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample size: [(60000, 784)]\n",
      "test sample size:[(10000, 784)]\n",
      "hidden units number: [100]\n",
      "batch_size:[1]\n",
      "epoch:1 | Training Accuracy:[0.9299666666666667]\n",
      "epoch:2 | Training Accuracy:[0.9691]\n",
      "epoch:3 | Training Accuracy:[0.9773833333333334]\n",
      "epoch:4 | Training Accuracy:[0.9828166666666667]\n",
      "epoch:5 | Training Accuracy:[0.9845]\n",
      "epoch:6 | Training Accuracy:[0.9875166666666667]\n",
      "epoch:7 | Training Accuracy:[0.9939333333333333]\n",
      "epoch:8 | Training Accuracy:[0.99495]\n",
      "epoch:9 | Training Accuracy:[0.9957]\n",
      "epoch:10 | Training Accuracy:[0.9963166666666666]\n",
      "epoch:11 | Training Accuracy:[0.9969166666666667]\n",
      "epoch:12 | Training Accuracy:[0.9970833333333333]\n",
      "epoch:13 | Training Accuracy:[0.9968333333333333]\n",
      "epoch:14 | Training Accuracy:[0.9972833333333333]\n",
      "epoch:15 | Training Accuracy:[0.9967833333333334]\n",
      "epoch:16 | Training Accuracy:[0.9971666666666666]\n",
      "epoch:17 | Training Accuracy:[0.9975333333333334]\n",
      "epoch:18 | Training Accuracy:[0.9972]\n",
      "epoch:19 | Training Accuracy:[0.9976333333333334]\n",
      "epoch:20 | Training Accuracy:[0.9976]\n"
     ]
    }
   ],
   "source": [
    "nn = MnistModel(x_train, y_train, x_test, y_test, hidden_units=100, batch_size=1, learning_rate=0.01, num_epochs=20, seed=1234)\n",
    "start = time.time()\n",
    "nn.train()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: [0.982]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: [{}]\".format(nn.test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
