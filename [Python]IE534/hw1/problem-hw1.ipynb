{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## Forward-Propagation\n",
    "\n",
    "$$\n",
    "X \\rightarrow Z=WX+b_1 \\rightarrow H=\\sigma(Z) \\rightarrow U = CH + b_2\\rightarrow S=F_{softmax}(U) \\rightarrow \\rho(S, y) = \\log S_y,\n",
    "$$\n",
    "where $S_y=\\frac{\\exp(U_y)}{\\sum_{j=0}^{K-1}\\exp(U_j)}$ is the y-th element of the $S$ and $U_y$ is the y-th element of the $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-Propagation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial U_t} =  \n",
    "\\begin{cases}\n",
    "S_t(U), & t\\neq y \\\\\n",
    "1- S_t(U). & t = y\n",
    "\\end{cases}\n",
    "\\Longrightarrow\n",
    "\\frac{\\partial \\rho}{\\partial U} = e_y - S(U), \n",
    "$$\n",
    "\n",
    "where $e_y$ is the unit vector, which y-th coordinate equals to 1 and 0 elsewhere. \n",
    "\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial \\rho}{\\partial b_2} = \\frac{\\partial \\rho}{\\partial U}\\frac{\\partial U}{\\partial b_2} = e_y - S(U) \\\\\n",
    "& \\frac{\\partial \\rho}{\\partial C} = \\frac{\\partial \\rho}{\\partial U}\\frac{\\partial U}{\\partial C} = (e_y - S(U))H^T \\\\\n",
    "& \\frac{\\partial \\rho}{\\partial H} = \\frac{\\partial \\rho}{\\partial U}\\frac{\\partial U}{\\partial H} = C^T\\frac{\\partial \\rho}{\\partial U} =C^T(e_y - S(U)) \\\\\n",
    "& \\frac{\\partial \\rho}{\\partial b_1} = \\frac{\\partial \\rho}{\\partial H}\\frac{\\partial H}{\\partial Z}\\frac{\\partial Z}{\\partial b_1} = \\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z)\\\\\n",
    "& \\frac{\\partial \\rho}{\\partial W} =  \\big(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z)\\big)X^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Mini-Batch Stochastic gradient algorithm for updating $\\theta = \\{W, b_1, C, b_2\\}$:\n",
    "* Step1: Specify batch_size $M$, activation function $\\sigma(z)$, and initialize $W^{(0)}, b_1^{(0)}, C^{(0)}, b_2^{(0)}$;\n",
    "* Step2: At iteration $t$:\n",
    "    * a. Select $M$ data samples $\\{X^{(t,m)},y^{(t,m)}\\}_{m=1}^M$ uniform at random from the full dataset $\\{X^{(n)},y^{(n)}\\}_{n=1}^N$ \n",
    "    * b. Compute forward-propagation:\n",
    "        * $Z^{(t,m)}=W^{(t)}X^{(t,m)}+b_1^{(t)}$\n",
    "        * $H^{(t,m)}=\\sigma(Z^{(t,m)})$\n",
    "        * $U^{(t,m)} = C^{(t)}H^{(t,m)} + b_2^{(t)}$\n",
    "        * $S^{(t,m)}=F_{softmax}(U^{(t,m)})$\n",
    "    * c. Compute backward-propagation:\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_2} = \\frac{1}{M}\\sum_{m=1}^M e_{y^{(t,m)}} - S^{(t,m)}$ \n",
    "        * $\\frac{\\partial \\rho}{\\partial C} =   \\frac{1}{M}\\sum_{m=1}^M  (e_{y^{(t,m)}}  - S^{(t,m)}){H^{(t,m)}}^T$\n",
    "        * $\\frac{\\partial \\rho}{\\partial H} = \\frac{1}{M}\\sum_{m=1}^M C^T(e_{y^{(t,m)}} - S^{(t,m)})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_1} = \\frac{1}{M}\\sum_{m=1}^M \\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t,m)})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial W} =  \\frac{1}{M}\\sum_{m=1}^M \\big(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t,m)})\\big){X^{(t,m)}}^T$\n",
    "    * Given learning rate $\\eta_t$, update parameters as follows:\n",
    "        * $b_2^{(t+1)}) \\leftarrow b_2^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_2}$\n",
    "        * $C^{(t+1)}) \\leftarrow C^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial C}$\n",
    "        * $b_1^{(t+1)}) \\leftarrow b_1^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_1}$\n",
    "        * $W^{(t+1)}) \\leftarrow W^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial W}$\n",
    "* Step3: Repeat Step2 until some convergence criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid unnecessary `for-loop` we can vectoruize the above algorithm. \n",
    "\n",
    "* Step1: Specify batch_size $M$, activation function $\\sigma(z)$, and initialize $W^{(0)}, b_1^{(0)}, C^{(0)}, b_2^{(0)}$;\n",
    "* Step2: At iteration $t$:\n",
    "    * a. Select $M$ data samples $\\{X^{(t,m)},y^{(t,m)}\\}_{m=1}^M$ uniform at random from the full dataset $\\{X^{(n)},y^{(n)}\\}_{n=1}^N$ \n",
    "    * b. Compute forward-propagation:\n",
    "        * $Z^{(t)}=W^{(t)}X^{(t)}+b_1^{(t)}$, where $X^{(t)} = (X^{(t,1)},...,X^{(t,M)})$ and the summation on $b_1$ will be column-wise.\n",
    "        * $H^{(t)}=\\sigma(Z^{(t)})$, where $H^{(t)} = (H^{(t,1)},...,H^{(t,M)})$ and $\\sigma(.)$ is element wise operation.\n",
    "        * $U^{(t)} = C^{(t)}H^{(t)} + b_2^{(t)}$\n",
    "        * $S^{(t)}=F_{softmax}(U^{(t)})$, where the $F_{softmax}$ is column-wise operation.\n",
    "    * c. Compute backward-propagation:\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_2} = \\text{np.mean}(e_{y^{(t)}} - S^{(t)}, \\text{axis=1})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial C} =  \\frac{1}{M} (e_{y^{(t)}}  - S^{(t)}){H^{(t)}}^T$\n",
    "        * $\\frac{\\partial \\rho}{\\partial H} = \\text{np.mean}(C^T(e_{y^{(t)}} - S^{(t)}), \\text{axis=1})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial b_1} = \\text{np.mean}(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t)}), \\text{axis=1})$\n",
    "        * $\\frac{\\partial \\rho}{\\partial W} =  \\frac{1}{M}(\\big(\\frac{\\partial \\rho}{\\partial H} \\odot \\sigma'(Z^{(t)})\\big){X^{(t)}}^T)$\n",
    "    * Given learning rate $\\eta_t$, update parameters as follows:\n",
    "        * $b_2^{(t+1)}) \\leftarrow b_2^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_2}$\n",
    "        * $C^{(t+1)}) \\leftarrow C^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial C}$\n",
    "        * $b_1^{(t+1)}) \\leftarrow b_1^{(t)}) + \\eta_t \\frac{\\partial \\rho}{\\partial b_1}$\n",
    "        * $W^{(t+1)}) \\leftarrow W^{(t)} + \\eta_t \\frac{\\partial \\rho}{\\partial W}$\n",
    "* Step3: Repeat Step2 until some convergence criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time \n",
    "import copy\n",
    "#import logging\n",
    "#from helperfunctions import create_log\n",
    "#logger = create_log(file_name=\"task.log\", log_level=logging.DEBUG)\n",
    "file_name = \"../data/MNISTdata.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.info(\"Load the MNIST dataset...\")\n",
    "data = h5py.File(file_name, \"r\")\n",
    "x_train = np.float32(data[\"x_train\"][:])\n",
    "y_train = np.int32(np.hstack(np.array(data[\"y_train\"])))\n",
    "x_test = np.float32(data[\"x_test\"][:])\n",
    "y_test = np.int32(np.hstack(np.array(data[\"y_test\"])))\n",
    "data.close()\n",
    "#logger.info(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel():\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, hidden_units=100, learning_rate=0.01, batch_size=20, num_epochs=5, seed=None):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.num_inputs = self.x_train.shape[1]\n",
    "        self.num_outputs = 10\n",
    "        self.hidden_units = hidden_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.params = {}\n",
    "        self.gradients = {}\n",
    "        if seed is not None:\n",
    "            r = np.random.RandomState(seed)\n",
    "            self.params[\"W\"] = r.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))  \n",
    "            self.params[\"C\"] = r.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1)) \n",
    "        else:\n",
    "            self.params[\"W\"] = np.random.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))\n",
    "            self.params[\"C\"] = np.random.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1))\n",
    "        print(\"training sample size: [{}]\\ntest sample size:[{}]\\nhidden units number: [{}]\\nbatch_size:[{}]\".format(self.x_train.shape, self.x_test.shape, self.hidden_units, self.batch_size))\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: x if x > 0 else 0, z)]\n",
    "\n",
    "    def activation_gradient(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: 1 if x > 0 else 0, z)]\n",
    "\n",
    "    def softmax(self, U):\n",
    "        temp = np.exp(U)\n",
    "        return temp / np.sum(temp)\n",
    "\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        random_index = np.random.choice(self.x_train.shape[0], replace=False, size=self.batch_size)\n",
    "        self.x_train_sub_samples = self.x_train[random_index].reshape((-1, self.batch_size))\n",
    "        self.y_train_sub_samples = self.y_train[random_index]\n",
    "        self.forward_results = {}\n",
    "        self.forward_results[\"Z\"] = np.dot(self.params[\"W\"], self.x_train_sub_samples) + self.params[\"b1\"]\n",
    "        self.forward_results[\"H\"] = np.apply_along_axis(self.activation, 0, self.forward_results[\"Z\"])\n",
    "        self.forward_results[\"U\"] = np.dot(self.params[\"C\"], self.forward_results[\"H\"]) + self.params[\"b2\"]\n",
    "        self.forward_results[\"S\"] = np.apply_along_axis(self.softmax, 0, self.forward_results[\"U\"])\n",
    "\n",
    "    def create_unit_matrix(self):\n",
    "        ey = np.zeros((self.num_outputs, self.batch_size))\n",
    "        for col_index, row_index in enumerate(self.y_train_sub_samples):\n",
    "            ey[row_index, col_index] = 1\n",
    "        return(ey)\n",
    "\n",
    "    def back_propagation(self):\n",
    "        ey = self.create_unit_matrix()\n",
    "        temp = - (ey - self.forward_results[\"S\"])\n",
    "        self.gradients[\"db2\"] = np.mean(temp, axis=1, keepdims=True)\n",
    "        self.gradients[\"dC\"] = np.dot(temp, self.forward_results[\"H\"].T) / self.batch_size\n",
    "        self.gradients[\"dH\"] = np.mean(np.dot(self.params[\"C\"].T, temp), axis=1, keepdims=True)\n",
    "        H_gradient = np.apply_along_axis(self.activation_gradient, 0, self.forward_results[\"Z\"])\n",
    "        temp2 = np.multiply(self.gradients[\"dH\"], H_gradient)\n",
    "        self.gradients[\"db1\"] = np.mean(temp2, axis=1, keepdims=True)\n",
    "        self.gradients[\"dW\"] = np.dot(temp2, self.x_train_sub_samples.T) / self.batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if (epoch > 5):\n",
    "                self.learning_rate = 0.001\n",
    "            if (epoch > 10):\n",
    "                self.learning_rate = 0.0001\n",
    "            if (epoch > 15):\n",
    "                self.learning_rate = 0.00001\n",
    "            total_correct = 0\n",
    "            for i in range(int(self.x_train.shape[0] / self.batch_size)):\n",
    "                self.forward_propagation()\n",
    "                prediction_train =  np.argmax(self.forward_results[\"S\"], axis=0)\n",
    "                total_correct += np.sum(prediction_train == self.y_train_sub_samples)\n",
    "                self.back_propagation()\n",
    "                self.params[\"W\"] -= self.learning_rate * self.gradients[\"dW\"]\n",
    "                self.params[\"b1\"] -= self.learning_rate * self.gradients[\"db1\"]\n",
    "                self.params[\"C\"] -= self.learning_rate * self.gradients[\"dC\"]\n",
    "                self.params[\"b2\"] -= self.learning_rate * self.gradients[\"db2\"]\n",
    "            print(\"epoch:{} | Training Accuracy:[{}]\".format(epoch+1, total_correct/len(self.x_train)))\n",
    "    def test(self):\n",
    "        self.Z = np.dot(self.params[\"W\"], self.x_test.T) + self.params[\"b1\"]\n",
    "        self.H = np.apply_along_axis(self.activation, 0, self.Z)\n",
    "        self.U = np.dot(self.params[\"C\"], self.H) + self.params[\"b2\"]\n",
    "        self.S = np.apply_along_axis(self.softmax, 0, self.U)\n",
    "        self.prediction = np.apply_along_axis(np.argmax, 0, self.S)\n",
    "        correct_ratio = np.mean(self.prediction == self.y_test)\n",
    "        return correct_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch_Size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample size: [(60000, 784)]\n",
      "test sample size:[(10000, 784)]\n",
      "hidden units number: [100]\n",
      "batch_size:[1]\n"
     ]
    }
   ],
   "source": [
    "nn = MnistModel(x_train, y_train, x_test, y_test, hidden_units=100, batch_size=1, learning_rate=0.01, num_epochs=5, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | Training Accuracy:[0.9296]\n",
      "epoch:2 | Training Accuracy:[0.96985]\n",
      "epoch:3 | Training Accuracy:[0.9783333333333334]\n",
      "epoch:4 | Training Accuracy:[0.9819166666666667]\n",
      "epoch:5 | Training Accuracy:[0.98605]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nn.train()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time: [672.2338092327118] second\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Time: [{}] second\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: [0.975]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: [{}]\".format(nn.test()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug happens when `batch_size>1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel():\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, hidden_units=100, learning_rate=0.01, batch_size=20, num_epochs=5, seed=None):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.num_inputs = self.x_train.shape[1]\n",
    "        self.num_outputs = 10\n",
    "        self.hidden_units = hidden_units\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.params = {}\n",
    "        self.gradients = {}\n",
    "        if seed is not None:\n",
    "            r = np.random.RandomState(seed)\n",
    "            self.params[\"W\"] = r.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))  \n",
    "            self.params[\"C\"] = r.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1)) \n",
    "        else:\n",
    "            self.params[\"W\"] = np.random.randn(self.hidden_units, self.num_inputs) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b1\"] = np.zeros((self.hidden_units, 1))\n",
    "            self.params[\"C\"] = np.random.randn(self.num_outputs, self.hidden_units) / np.sqrt(self.num_inputs)\n",
    "            self.params[\"b2\"] = np.zeros((self.num_outputs, 1))\n",
    "        print(\"training sample size: [{}]\\ntest sample size:[{}]\\nhidden units number: [{}]\\nbatch_size:[{}]\".format(self.x_train.shape, self.x_test.shape, self.hidden_units, self.batch_size))\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: x if x > 0 else 0, z)]\n",
    "\n",
    "    def activation_gradient(self, z):\n",
    "        \"\"\"\n",
    "        z: must be of size (hidden_units * 1)\n",
    "        \"\"\"\n",
    "        return [*map(lambda x: 1 if x > 0 else 0, z)]\n",
    "\n",
    "    def softmax(self, U):\n",
    "        temp = np.exp(U)\n",
    "        return temp / np.sum(temp)\n",
    "\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        random_index = np.random.choice(self.x_train.shape[0], replace=False, size=self.batch_size)\n",
    "        self.x_train_sub_samples = self.x_train[random_index].reshape((-1, self.batch_size))\n",
    "        self.y_train_sub_samples = self.y_train[random_index]\n",
    "        self.forward_results = {}\n",
    "        self.forward_results[\"Z\"] = np.dot(self.params[\"W\"], self.x_train_sub_samples) + self.params[\"b1\"]\n",
    "        self.forward_results[\"H\"] = np.apply_along_axis(self.activation, 0, self.forward_results[\"Z\"])\n",
    "        self.forward_results[\"U\"] = np.dot(self.params[\"C\"], self.forward_results[\"H\"]) + self.params[\"b2\"]\n",
    "        self.forward_results[\"S\"] = np.apply_along_axis(self.softmax, 0, self.forward_results[\"U\"])\n",
    "\n",
    "    def create_unit_matrix(self):\n",
    "        ey = np.zeros((self.num_outputs, self.batch_size))\n",
    "        for col_index, row_index in enumerate(self.y_train_sub_samples):\n",
    "            ey[row_index, col_index] = 1\n",
    "        return(ey)\n",
    "\n",
    "    def back_propagation(self):\n",
    "        ey = self.create_unit_matrix()\n",
    "        temp = - (ey - self.forward_results[\"S\"])\n",
    "        self.gradients[\"db2\"] = np.mean(temp, axis=1, keepdims=True)\n",
    "        self.gradients[\"dC\"] = np.dot(temp, self.forward_results[\"H\"].T) / self.batch_size\n",
    "        temp2 = np.dot(self.params[\"C\"].T, temp)\n",
    "        self.gradients[\"dH\"] = np.mean(temp2, axis=1, keepdims=True)\n",
    "        H_gradient = np.apply_along_axis(self.activation_gradient, 0, self.forward_results[\"Z\"])\n",
    "        temp3 = np.multiply(temp2, H_gradient)\n",
    "        self.gradients[\"db1\"] = np.mean(temp3, axis=1, keepdims=True)\n",
    "        #self.gradients[\"dW\"] = np.dot(temp3, self.x_train_sub_samples.T) / self.batch_size\n",
    "        self.gradients[\"dW\"] = 0\n",
    "        for i in range(int(temp3.shape[1])):\n",
    "            #print(temp3[i].reshape(-1,1).shape, self.x_train_sub_samples.shape)\n",
    "            self.gradients[\"dW\"] += np.dot(temp3[i].reshape(-1,1), self.x_train_sub_samples[:,i].reshape(1, self.num_inputs)) \n",
    "        self.gradients[\"dW\"] /= self.batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if (epoch > 5):\n",
    "                self.learning_rate = 0.001\n",
    "            if (epoch > 10):\n",
    "                self.learning_rate = 0.0001\n",
    "            if (epoch > 15):\n",
    "                self.learning_rate = 0.00001\n",
    "            total_correct = 0\n",
    "            for i in range(int(self.x_train.shape[0] / self.batch_size)):\n",
    "                self.forward_propagation()\n",
    "                prediction_train =  np.argmax(self.forward_results[\"S\"], axis=0)\n",
    "                total_correct += np.sum(prediction_train == self.y_train_sub_samples)\n",
    "                self.back_propagation()\n",
    "                #print(self.params[\"W\"].shape, self.gradients[\"dW\"].shape)\n",
    "                self.params[\"W\"] -= self.learning_rate * self.gradients[\"dW\"]\n",
    "                self.params[\"b1\"] -= self.learning_rate * self.gradients[\"db1\"]\n",
    "                self.params[\"C\"] -= self.learning_rate * self.gradients[\"dC\"]\n",
    "                self.params[\"b2\"] -= self.learning_rate * self.gradients[\"db2\"]\n",
    "            print(\"epoch:{} | Training Accuracy:[{}]\".format(epoch+1, total_correct/len(self.x_train)))\n",
    "    def test(self):\n",
    "        self.Z = np.dot(self.params[\"W\"], self.x_test.T) + self.params[\"b1\"]\n",
    "        self.H = np.apply_along_axis(self.activation, 0, self.Z)\n",
    "        self.U = np.dot(self.params[\"C\"], self.H) + self.params[\"b2\"]\n",
    "        self.S = np.apply_along_axis(self.softmax, 0, self.U)\n",
    "        self.prediction = np.apply_along_axis(np.argmax, 0, self.S)\n",
    "        correct_ratio = np.mean(self.prediction == self.y_test)\n",
    "        return correct_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample size: [(60000, 784)]\n",
      "test sample size:[(10000, 784)]\n",
      "hidden units number: [100]\n",
      "batch_size:[100]\n"
     ]
    }
   ],
   "source": [
    "nn = MnistModel(x_train, y_train, x_test, y_test, hidden_units=100, batch_size=100, learning_rate=0.01, num_epochs=20, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | Training Accuracy:[0.10266666666666667]\n",
      "epoch:2 | Training Accuracy:[0.10678333333333333]\n",
      "epoch:3 | Training Accuracy:[0.10898333333333333]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-60c85645c3d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-948ecff89fbf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mprediction_train\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"S\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train_sub_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0;31m#print(self.params[\"W\"].shape, self.gradients[\"dW\"].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-948ecff89fbf>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtemp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mH_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mtemp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-948ecff89fbf>\u001b[0m in \u001b[0;36mactivation_gradient\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mof\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-948ecff89fbf>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mof\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [6, 8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.array([[1],[2]])\n",
    "np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
